{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6905bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Linear Agent...\n",
      "\n",
      "Starting seed 0 for best linear.pt\n",
      "Seed 0 Ep 50/500  recent_avg=-389.96  eps=0.778  replay=17037\n",
      "Seed 0 Ep 100/500  recent_avg=-2282.78  eps=0.606  replay=20000\n",
      "Seed 0 Ep 150/500  recent_avg=-54.14  eps=0.471  replay=20000\n",
      "Seed 0 Ep 200/500  recent_avg=-67.46  eps=0.367  replay=20000\n",
      "Seed 0 Ep 250/500  recent_avg=-50.88  eps=0.286  replay=20000\n",
      "Seed 0 Ep 300/500  recent_avg=-50.58  eps=0.222  replay=20000\n",
      "Seed 0 Ep 350/500  recent_avg=-50.50  eps=0.173  replay=20000\n",
      "Seed 0 Ep 400/500  recent_avg=-50.54  eps=0.135  replay=20000\n",
      "Seed 0 Ep 450/500  recent_avg=-50.40  eps=0.105  replay=20000\n",
      "Seed 0 Ep 500/500  recent_avg=-50.04  eps=0.082  replay=20000\n",
      "Seed 0 finished. avg_last100=-50.22\n",
      "Saved new best model to Q1/models\\best linear.pt (avg_last100=-50.22)\n",
      "\n",
      "Starting seed 1 for best linear.pt\n",
      "Seed 1 Ep 50/500  recent_avg=-72.00  eps=0.778  replay=1143\n",
      "Seed 1 Ep 100/500  recent_avg=-70.78  eps=0.606  replay=2231\n",
      "Seed 1 Ep 150/500  recent_avg=-1780.14  eps=0.471  replay=20000\n",
      "Seed 1 Ep 200/500  recent_avg=-115.22  eps=0.367  replay=20000\n",
      "Seed 1 Ep 250/500  recent_avg=-71.02  eps=0.286  replay=20000\n",
      "Seed 1 Ep 300/500  recent_avg=-69.18  eps=0.222  replay=20000\n",
      "Seed 1 Ep 350/500  recent_avg=-52.54  eps=0.173  replay=20000\n",
      "Seed 1 Ep 400/500  recent_avg=-50.22  eps=0.135  replay=20000\n",
      "Seed 1 Ep 450/500  recent_avg=-64.36  eps=0.105  replay=20000\n",
      "Seed 1 Ep 500/500  recent_avg=-50.06  eps=0.082  replay=20000\n",
      "Seed 1 finished. avg_last100=-57.21\n",
      "\n",
      "Starting seed 2 for best linear.pt\n",
      "Seed 2 Ep 50/500  recent_avg=-70.74  eps=0.778  replay=1086\n",
      "Seed 2 Ep 100/500  recent_avg=-59.80  eps=0.606  replay=1626\n",
      "Seed 2 Ep 150/500  recent_avg=-2455.44  eps=0.471  replay=20000\n",
      "Seed 2 Ep 200/500  recent_avg=-62.46  eps=0.367  replay=20000\n",
      "Seed 2 Ep 250/500  recent_avg=-52.08  eps=0.286  replay=20000\n",
      "Seed 2 Ep 300/500  recent_avg=-50.24  eps=0.222  replay=20000\n",
      "Seed 2 Ep 350/500  recent_avg=-51.04  eps=0.173  replay=20000\n",
      "Seed 2 Ep 400/500  recent_avg=-52.68  eps=0.135  replay=20000\n",
      "Seed 2 Ep 450/500  recent_avg=-50.66  eps=0.105  replay=20000\n",
      "Seed 2 Ep 500/500  recent_avg=-50.30  eps=0.082  replay=20000\n",
      "Seed 2 finished. avg_last100=-50.48\n",
      "\n",
      "Starting seed 3 for best linear.pt\n",
      "Seed 3 Ep 50/500  recent_avg=-968.40  eps=0.778  replay=20000\n",
      "Seed 3 Ep 100/500  recent_avg=-1613.26  eps=0.606  replay=20000\n",
      "Seed 3 Ep 150/500  recent_avg=-52.08  eps=0.471  replay=20000\n",
      "Seed 3 Ep 200/500  recent_avg=-88.54  eps=0.367  replay=20000\n",
      "Seed 3 Ep 250/500  recent_avg=-61.30  eps=0.286  replay=20000\n",
      "Seed 3 Ep 300/500  recent_avg=-50.48  eps=0.222  replay=20000\n",
      "Seed 3 Ep 350/500  recent_avg=-50.54  eps=0.173  replay=20000\n",
      "Seed 3 Ep 400/500  recent_avg=-50.86  eps=0.135  replay=20000\n",
      "Seed 3 Ep 450/500  recent_avg=-50.26  eps=0.105  replay=20000\n",
      "Seed 3 Ep 500/500  recent_avg=-50.18  eps=0.082  replay=20000\n",
      "Seed 3 finished. avg_last100=-50.22\n",
      "\n",
      "Starting seed 4 for best linear.pt\n",
      "Seed 4 Ep 50/500  recent_avg=-72.78  eps=0.778  replay=1184\n",
      "Seed 4 Ep 100/500  recent_avg=-55.00  eps=0.606  replay=1484\n",
      "Seed 4 Ep 150/500  recent_avg=-57.82  eps=0.471  replay=1925\n",
      "Seed 4 Ep 200/500  recent_avg=-57.90  eps=0.367  replay=2370\n",
      "Seed 4 Ep 250/500  recent_avg=-80.04  eps=0.286  replay=3922\n",
      "Seed 4 Ep 300/500  recent_avg=-2090.66  eps=0.222  replay=20000\n",
      "Seed 4 Ep 350/500  recent_avg=-89.98  eps=0.173  replay=20000\n",
      "Seed 4 Ep 400/500  recent_avg=-50.18  eps=0.135  replay=20000\n",
      "Seed 4 Ep 450/500  recent_avg=-53.92  eps=0.105  replay=20000\n",
      "Seed 4 Ep 500/500  recent_avg=-50.04  eps=0.082  replay=20000\n",
      "Seed 4 finished. avg_last100=-51.98\n",
      "\n",
      "Starting seed 5 for best linear.pt\n",
      "Seed 5 Ep 50/500  recent_avg=-69.14  eps=0.778  replay=995\n",
      "Seed 5 Ep 100/500  recent_avg=-2031.04  eps=0.606  replay=20000\n",
      "Seed 5 Ep 150/500  recent_avg=-136.44  eps=0.471  replay=20000\n",
      "Seed 5 Ep 200/500  recent_avg=-79.86  eps=0.367  replay=20000\n",
      "Seed 5 Ep 250/500  recent_avg=-58.06  eps=0.286  replay=20000\n",
      "Seed 5 Ep 300/500  recent_avg=-52.78  eps=0.222  replay=20000\n",
      "Seed 5 Ep 350/500  recent_avg=-50.82  eps=0.173  replay=20000\n",
      "Seed 5 Ep 400/500  recent_avg=-55.14  eps=0.135  replay=20000\n",
      "Seed 5 Ep 450/500  recent_avg=-61.02  eps=0.105  replay=20000\n",
      "Seed 5 Ep 500/500  recent_avg=-50.34  eps=0.082  replay=20000\n",
      "Seed 5 finished. avg_last100=-55.68\n",
      "\n",
      "Starting seed 6 for best linear.pt\n",
      "Seed 6 Ep 50/500  recent_avg=-75.16  eps=0.778  replay=1298\n",
      "Seed 6 Ep 100/500  recent_avg=-2248.46  eps=0.606  replay=20000\n",
      "Seed 6 Ep 150/500  recent_avg=-186.52  eps=0.471  replay=20000\n",
      "Seed 6 Ep 200/500  recent_avg=-54.52  eps=0.367  replay=20000\n",
      "Seed 6 Ep 250/500  recent_avg=-66.88  eps=0.286  replay=20000\n",
      "Seed 6 Ep 300/500  recent_avg=-53.04  eps=0.222  replay=20000\n",
      "Seed 6 Ep 350/500  recent_avg=-50.48  eps=0.173  replay=20000\n",
      "Seed 6 Ep 400/500  recent_avg=-50.28  eps=0.135  replay=20000\n",
      "Seed 6 Ep 450/500  recent_avg=-50.06  eps=0.105  replay=20000\n",
      "Seed 6 Ep 500/500  recent_avg=-50.06  eps=0.082  replay=20000\n",
      "Seed 6 finished. avg_last100=-50.06\n",
      "Saved new best model to Q1/models\\best linear.pt (avg_last100=-50.06)\n",
      "\n",
      "Starting seed 7 for best linear.pt\n",
      "Seed 7 Ep 50/500  recent_avg=-459.42  eps=0.778  replay=20000\n",
      "Seed 7 Ep 100/500  recent_avg=-1864.18  eps=0.606  replay=20000\n",
      "Seed 7 Ep 150/500  recent_avg=-107.58  eps=0.471  replay=20000\n",
      "Seed 7 Ep 200/500  recent_avg=-50.16  eps=0.367  replay=20000\n",
      "Seed 7 Ep 250/500  recent_avg=-62.02  eps=0.286  replay=20000\n",
      "Seed 7 Ep 300/500  recent_avg=-50.20  eps=0.222  replay=20000\n",
      "Seed 7 Ep 350/500  recent_avg=-63.78  eps=0.173  replay=20000\n",
      "Seed 7 Ep 400/500  recent_avg=-50.12  eps=0.135  replay=20000\n",
      "Seed 7 Ep 450/500  recent_avg=-50.34  eps=0.105  replay=20000\n",
      "Seed 7 Ep 500/500  recent_avg=-50.64  eps=0.082  replay=20000\n",
      "Seed 7 finished. avg_last100=-50.49\n",
      "\n",
      "Starting seed 8 for best linear.pt\n",
      "Seed 8 Ep 50/500  recent_avg=-59.34  eps=0.778  replay=507\n",
      "Seed 8 Ep 100/500  recent_avg=-56.44  eps=0.606  replay=878\n",
      "Seed 8 Ep 150/500  recent_avg=-52.22  eps=0.471  replay=1039\n",
      "Seed 8 Ep 200/500  recent_avg=-51.46  eps=0.367  replay=1162\n",
      "Seed 8 Ep 250/500  recent_avg=-51.74  eps=0.286  replay=1299\n",
      "Seed 8 Ep 300/500  recent_avg=-56.12  eps=0.222  replay=1655\n",
      "Seed 8 Ep 350/500  recent_avg=-50.16  eps=0.173  replay=1713\n",
      "Seed 8 Ep 400/500  recent_avg=-50.20  eps=0.135  replay=1773\n",
      "Seed 8 Ep 450/500  recent_avg=-50.14  eps=0.105  replay=1830\n",
      "Seed 8 Ep 500/500  recent_avg=-2658.64  eps=0.082  replay=20000\n",
      "Seed 8 finished. avg_last100=-1354.39\n",
      "\n",
      "Starting seed 9 for best linear.pt\n",
      "Seed 9 Ep 50/500  recent_avg=-59.46  eps=0.778  replay=521\n",
      "Seed 9 Ep 100/500  recent_avg=-55.38  eps=0.606  replay=840\n",
      "Seed 9 Ep 150/500  recent_avg=-55.86  eps=0.471  replay=1183\n",
      "Seed 9 Ep 200/500  recent_avg=-69.40  eps=0.367  replay=2203\n",
      "Seed 9 Ep 250/500  recent_avg=-67.56  eps=0.286  replay=3131\n",
      "Seed 9 Ep 300/500  recent_avg=-2941.94  eps=0.222  replay=20000\n",
      "Seed 9 Ep 350/500  recent_avg=-53.68  eps=0.173  replay=20000\n",
      "Seed 9 Ep 400/500  recent_avg=-50.04  eps=0.135  replay=20000\n",
      "Seed 9 Ep 450/500  recent_avg=-51.06  eps=0.105  replay=20000\n",
      "Seed 9 Ep 500/500  recent_avg=-50.14  eps=0.082  replay=20000\n",
      "Seed 9 finished. avg_last100=-50.60\n",
      "Training Non-Linear Agent...\n",
      "\n",
      "Starting seed 0 for best nonlinear.pt\n",
      "Seed 0 Ep 50/500  recent_avg=-72.38  eps=0.778  replay=1167\n",
      "Seed 0 Ep 100/500  recent_avg=-88.38  eps=0.606  replay=3202\n",
      "Seed 0 Ep 150/500  recent_avg=-93.34  eps=0.471  replay=5409\n",
      "Seed 0 Ep 200/500  recent_avg=-189.02  eps=0.367  replay=12631\n",
      "Seed 0 Ep 250/500  recent_avg=-75.12  eps=0.286  replay=13937\n",
      "Seed 0 Ep 300/500  recent_avg=-97.72  eps=0.222  replay=16611\n",
      "Seed 0 Ep 350/500  recent_avg=-69.40  eps=0.173  replay=17631\n",
      "Seed 0 Ep 400/500  recent_avg=-94.22  eps=0.135  replay=19891\n",
      "Seed 0 Ep 450/500  recent_avg=-147.92  eps=0.105  replay=20000\n",
      "Seed 0 Ep 500/500  recent_avg=-113.74  eps=0.082  replay=20000\n",
      "Seed 0 finished. avg_last100=-130.83\n",
      "Saved new best model to Q1/models\\best nonlinear.pt (avg_last100=-130.83)\n",
      "\n",
      "Starting seed 1 for best nonlinear.pt\n",
      "Seed 1 Ep 50/500  recent_avg=-57.38  eps=0.778  replay=418\n",
      "Seed 1 Ep 100/500  recent_avg=-87.08  eps=0.606  replay=2307\n",
      "Seed 1 Ep 150/500  recent_avg=-105.84  eps=0.471  replay=5390\n",
      "Seed 1 Ep 200/500  recent_avg=-87.26  eps=0.367  replay=7303\n",
      "Seed 1 Ep 250/500  recent_avg=-70.82  eps=0.286  replay=8393\n",
      "Seed 1 Ep 300/500  recent_avg=-85.44  eps=0.222  replay=10215\n",
      "Seed 1 Ep 350/500  recent_avg=-72.16  eps=0.173  replay=11371\n",
      "Seed 1 Ep 400/500  recent_avg=-77.34  eps=0.135  replay=12788\n",
      "Seed 1 Ep 450/500  recent_avg=-84.46  eps=0.105  replay=14561\n",
      "Seed 1 Ep 500/500  recent_avg=-138.34  eps=0.082  replay=18988\n",
      "Seed 1 finished. avg_last100=-111.40\n",
      "Saved new best model to Q1/models\\best nonlinear.pt (avg_last100=-111.40)\n",
      "\n",
      "Starting seed 2 for best nonlinear.pt\n",
      "Seed 2 Ep 50/500  recent_avg=-90.72  eps=0.778  replay=2064\n",
      "Seed 2 Ep 100/500  recent_avg=-61.56  eps=0.606  replay=2690\n",
      "Seed 2 Ep 150/500  recent_avg=-178.00  eps=0.471  replay=9129\n",
      "Seed 2 Ep 200/500  recent_avg=-99.20  eps=0.367  replay=11626\n",
      "Seed 2 Ep 250/500  recent_avg=-311.82  eps=0.286  replay=20000\n",
      "Seed 2 Ep 300/500  recent_avg=-234.42  eps=0.222  replay=20000\n",
      "Seed 2 Ep 350/500  recent_avg=-385.52  eps=0.173  replay=20000\n",
      "Seed 2 Ep 400/500  recent_avg=-122.26  eps=0.135  replay=20000\n",
      "Seed 2 Ep 450/500  recent_avg=-245.78  eps=0.105  replay=20000\n",
      "Seed 2 Ep 500/500  recent_avg=-131.22  eps=0.082  replay=20000\n",
      "Seed 2 finished. avg_last100=-188.50\n",
      "\n",
      "Starting seed 3 for best nonlinear.pt\n",
      "Seed 3 Ep 50/500  recent_avg=-82.68  eps=0.778  replay=1671\n",
      "Seed 3 Ep 100/500  recent_avg=-78.64  eps=0.606  replay=3143\n",
      "Seed 3 Ep 150/500  recent_avg=-165.20  eps=0.471  replay=8928\n",
      "Seed 3 Ep 200/500  recent_avg=-113.84  eps=0.367  replay=12159\n",
      "Seed 3 Ep 250/500  recent_avg=-215.34  eps=0.286  replay=20000\n",
      "Seed 3 Ep 300/500  recent_avg=-335.90  eps=0.222  replay=20000\n",
      "Seed 3 Ep 350/500  recent_avg=-813.48  eps=0.173  replay=20000\n",
      "Seed 3 Ep 400/500  recent_avg=-51.16  eps=0.135  replay=20000\n",
      "Seed 3 Ep 450/500  recent_avg=-50.34  eps=0.105  replay=20000\n",
      "Seed 3 Ep 500/500  recent_avg=-50.14  eps=0.082  replay=20000\n",
      "Seed 3 finished. avg_last100=-50.24\n",
      "Saved new best model to Q1/models\\best nonlinear.pt (avg_last100=-50.24)\n",
      "\n",
      "Starting seed 4 for best nonlinear.pt\n",
      "Seed 4 Ep 50/500  recent_avg=-110.50  eps=0.778  replay=3043\n",
      "Seed 4 Ep 100/500  recent_avg=-135.44  eps=0.606  replay=7357\n",
      "Seed 4 Ep 150/500  recent_avg=-71.90  eps=0.471  replay=8750\n",
      "Seed 4 Ep 200/500  recent_avg=-351.14  eps=0.367  replay=20000\n",
      "Seed 4 Ep 250/500  recent_avg=-286.10  eps=0.286  replay=20000\n",
      "Seed 4 Ep 300/500  recent_avg=-147.94  eps=0.222  replay=20000\n",
      "Seed 4 Ep 350/500  recent_avg=-199.12  eps=0.173  replay=20000\n",
      "Seed 4 Ep 400/500  recent_avg=-243.38  eps=0.135  replay=20000\n",
      "Seed 4 Ep 450/500  recent_avg=-107.84  eps=0.105  replay=20000\n",
      "Seed 4 Ep 500/500  recent_avg=-126.50  eps=0.082  replay=20000\n",
      "Seed 4 finished. avg_last100=-117.17\n",
      "\n",
      "Starting seed 5 for best nonlinear.pt\n",
      "Seed 5 Ep 50/500  recent_avg=-55.58  eps=0.778  replay=329\n",
      "Seed 5 Ep 100/500  recent_avg=-132.92  eps=0.606  replay=4505\n",
      "Seed 5 Ep 150/500  recent_avg=-195.88  eps=0.471  replay=11834\n",
      "Seed 5 Ep 200/500  recent_avg=-120.06  eps=0.367  replay=15384\n",
      "Seed 5 Ep 250/500  recent_avg=-69.52  eps=0.286  replay=16410\n",
      "Seed 5 Ep 300/500  recent_avg=-83.08  eps=0.222  replay=18113\n",
      "Seed 5 Ep 350/500  recent_avg=-75.24  eps=0.173  replay=19419\n",
      "Seed 5 Ep 400/500  recent_avg=-90.46  eps=0.135  replay=20000\n",
      "Seed 5 Ep 450/500  recent_avg=-293.08  eps=0.105  replay=20000\n",
      "Seed 5 Ep 500/500  recent_avg=-495.80  eps=0.082  replay=20000\n",
      "Seed 5 finished. avg_last100=-394.44\n",
      "\n",
      "Starting seed 6 for best nonlinear.pt\n",
      "Seed 6 Ep 50/500  recent_avg=-79.72  eps=0.778  replay=1529\n",
      "Seed 6 Ep 100/500  recent_avg=-91.98  eps=0.606  replay=3664\n",
      "Seed 6 Ep 150/500  recent_avg=-352.16  eps=0.471  replay=18793\n",
      "Seed 6 Ep 200/500  recent_avg=-84.98  eps=0.367  replay=20000\n",
      "Seed 6 Ep 250/500  recent_avg=-92.80  eps=0.286  replay=20000\n",
      "Seed 6 Ep 300/500  recent_avg=-101.04  eps=0.222  replay=20000\n",
      "Seed 6 Ep 350/500  recent_avg=-281.82  eps=0.173  replay=20000\n",
      "Seed 6 Ep 400/500  recent_avg=-157.18  eps=0.135  replay=20000\n",
      "Seed 6 Ep 450/500  recent_avg=-579.52  eps=0.105  replay=20000\n",
      "Seed 6 Ep 500/500  recent_avg=-92.42  eps=0.082  replay=20000\n",
      "Seed 6 finished. avg_last100=-335.97\n",
      "\n",
      "Starting seed 7 for best nonlinear.pt\n",
      "Seed 7 Ep 50/500  recent_avg=-55.94  eps=0.778  replay=346\n",
      "Seed 7 Ep 100/500  recent_avg=-98.40  eps=0.606  replay=3131\n",
      "Seed 7 Ep 150/500  recent_avg=-226.10  eps=0.471  replay=12212\n",
      "Seed 7 Ep 200/500  recent_avg=-195.46  eps=0.367  replay=19517\n",
      "Seed 7 Ep 250/500  recent_avg=-80.16  eps=0.286  replay=20000\n",
      "Seed 7 Ep 300/500  recent_avg=-68.98  eps=0.222  replay=20000\n",
      "Seed 7 Ep 350/500  recent_avg=-111.72  eps=0.173  replay=20000\n",
      "Seed 7 Ep 400/500  recent_avg=-108.34  eps=0.135  replay=20000\n",
      "Seed 7 Ep 450/500  recent_avg=-116.00  eps=0.105  replay=20000\n",
      "Seed 7 Ep 500/500  recent_avg=-114.66  eps=0.082  replay=20000\n",
      "Seed 7 finished. avg_last100=-115.33\n",
      "\n",
      "Starting seed 8 for best nonlinear.pt\n",
      "Seed 8 Ep 50/500  recent_avg=-78.10  eps=0.778  replay=1435\n",
      "Seed 8 Ep 100/500  recent_avg=-95.00  eps=0.606  replay=3732\n",
      "Seed 8 Ep 150/500  recent_avg=-157.56  eps=0.471  replay=9387\n",
      "Seed 8 Ep 200/500  recent_avg=-316.34  eps=0.367  replay=20000\n",
      "Seed 8 Ep 250/500  recent_avg=-107.60  eps=0.286  replay=20000\n",
      "Seed 8 Ep 300/500  recent_avg=-102.40  eps=0.222  replay=20000\n",
      "Seed 8 Ep 350/500  recent_avg=-115.74  eps=0.173  replay=20000\n",
      "Seed 8 Ep 400/500  recent_avg=-105.92  eps=0.135  replay=20000\n",
      "Seed 8 Ep 450/500  recent_avg=-90.48  eps=0.105  replay=20000\n",
      "Seed 8 Ep 500/500  recent_avg=-135.06  eps=0.082  replay=20000\n",
      "Seed 8 finished. avg_last100=-112.77\n",
      "\n",
      "Starting seed 9 for best nonlinear.pt\n",
      "Seed 9 Ep 50/500  recent_avg=-98.36  eps=0.778  replay=2532\n",
      "Seed 9 Ep 100/500  recent_avg=-88.42  eps=0.606  replay=4493\n",
      "Seed 9 Ep 150/500  recent_avg=-280.06  eps=0.471  replay=16028\n",
      "Seed 9 Ep 200/500  recent_avg=-160.04  eps=0.367  replay=20000\n",
      "Seed 9 Ep 250/500  recent_avg=-125.34  eps=0.286  replay=20000\n",
      "Seed 9 Ep 300/500  recent_avg=-72.98  eps=0.222  replay=20000\n",
      "Seed 9 Ep 350/500  recent_avg=-72.02  eps=0.173  replay=20000\n",
      "Seed 9 Ep 400/500  recent_avg=-103.68  eps=0.135  replay=20000\n",
      "Seed 9 Ep 450/500  recent_avg=-549.24  eps=0.105  replay=20000\n",
      "Seed 9 Ep 500/500  recent_avg=-331.28  eps=0.082  replay=20000\n",
      "Seed 9 finished. avg_last100=-440.26\n",
      "Evaluation...\n",
      "Done. Models, plots, and evaluation results saved under Q1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_9548\\3895142728.py:349: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  linear_model.load_state_dict(torch.load(os.path.join(MODEL_DIR, \"best linear.pt\"), map_location=DEVICE))\n",
      "C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_9548\\3895142728.py:357: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  nonlinear_model.load_state_dict(torch.load(os.path.join(MODEL_DIR, \"best nonlinear.pt\"), map_location=DEVICE))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from collections import deque, namedtuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from cliff import MultiGoalCliffWalkingEnv\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def state_to_tensor(state, env):\n",
    "    \"\"\"Converts the environment's discrete state into a feature tensor.\"\"\"\n",
    "    grid_size = env.height * env.width\n",
    "    checkpoint_status = state // grid_size\n",
    "    position_index = state % grid_size\n",
    "    \n",
    "    y = position_index // env.width\n",
    "    x = position_index % env.width\n",
    "\n",
    "    checkpoints_binary = [(checkpoint_status >> i) & 1 for i in range(2)]\n",
    "    \n",
    "    features = [y / env.height, x / env.width] + checkpoints_binary\n",
    "    return torch.tensor(features, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "NUM_SEEDS = 10\n",
    "EPISODES_PER_SEED = 500\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_CAPACITY = 20000\n",
    "GAMMA = 0.99\n",
    "LR = 1e-3\n",
    "TARGET_UPDATE_FREQ = 1000   # steps\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 0.995           # multiplicative per episode\n",
    "MIN_REPLAY_SIZE = 500\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Output folders\n",
    "MODEL_DIR = \"Q1/models\"\n",
    "PLOT_DIR = \"Q1/plots\"\n",
    "EVAL_DIR = \"Q1/evaluation\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "os.makedirs(EVAL_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------------- Utilities --------------------------------\n",
    "\n",
    "def decode_state_index(idx, env):\n",
    "    \"\"\"Convert discrete environment state index into a 4-dim vector:\n",
    "    [row_norm, col_norm, checkA, checkB]\n",
    "    where row_norm and col_norm are in [0,1].\n",
    "    \"\"\"\n",
    "    # idx encodes: base = r * width + c; checkpoint_bits = 0..3\n",
    "    width = env.width\n",
    "    base = idx // 4\n",
    "    checkpoint_bits = idx % 4\n",
    "    r = base // width\n",
    "    c = base % width\n",
    "    checkA = (checkpoint_bits >> 1) & 1\n",
    "    checkB = checkpoint_bits & 1\n",
    "    row_norm = r / float(env.height - 1)\n",
    "    col_norm = c / float(env.width - 1)\n",
    "    return np.array([row_norm, col_norm, float(checkA), float(checkB)], dtype=np.float32)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.experience = namedtuple(\"Experience\", [\"s\", \"a\", \"r\", \"ns\", \"d\"]) \n",
    "\n",
    "    def push(self, s, a, r, ns, d):\n",
    "        self.buffer.append(self.experience(s, a, r, ns, d))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states = torch.as_tensor(np.stack([b.s for b in batch]), dtype=torch.float32).to(DEVICE)\n",
    "        actions = torch.as_tensor(np.array([b.a for b in batch]), dtype=torch.int64).unsqueeze(1).to(DEVICE)\n",
    "        rewards = torch.as_tensor(np.array([b.r for b in batch]), dtype=torch.float32).unsqueeze(1).to(DEVICE)\n",
    "        next_states = torch.as_tensor(np.stack([b.ns for b in batch]), dtype=torch.float32).to(DEVICE)\n",
    "        dones = torch.as_tensor(np.array([b.d for b in batch]).astype(np.float32)).unsqueeze(1).to(DEVICE)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# ----------------------------- Networks ---------------------------------\n",
    "class LinearDQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearDQN, self).__init__()\n",
    "        # Single linear layer (no hidden) mapping input to Q-values\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        # small init\n",
    "        nn.init.kaiming_uniform_(self.fc.weight, nonlinearity='linear')\n",
    "        nn.init.constant_(self.fc.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class NonLinearDQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(NonLinearDQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "        # init\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "        # tiny init for final layer\n",
    "        final = self.net[-1]\n",
    "        if isinstance(final, nn.Linear):\n",
    "            nn.init.uniform_(final.weight, -1e-3, 1e-3)\n",
    "            nn.init.constant_(final.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# -------------------------- Trainer Class -------------------------------\n",
    "class DQNTrainer:\n",
    "    def __init__(self, env, network_class, lr=LR, seed=0):\n",
    "        self.env = env\n",
    "        self.seed = seed\n",
    "        set_seed(seed)\n",
    "        self.input_dim = 4\n",
    "        self.output_dim = env.action_space.n\n",
    "\n",
    "        self.policy_net = network_class(self.input_dim, self.output_dim).to(DEVICE)\n",
    "        self.target_net = network_class(self.input_dim, self.output_dim).to(DEVICE)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.replay = ReplayBuffer(BUFFER_CAPACITY)\n",
    "\n",
    "        self.steps_done = 0\n",
    "        self.epsilon = EPS_START\n",
    "        self.loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "    def select_action(self, state, explore=True):\n",
    "        # state: numpy array (input_dim,)\n",
    "        if explore:\n",
    "            # epsilon-greedy (per-call multiplicative decay outside if desired)\n",
    "            if random.random() > self.epsilon:\n",
    "                # inference - set eval to avoid BN issues\n",
    "                self.policy_net.eval()\n",
    "                with torch.no_grad():\n",
    "                    s = torch.as_tensor(state, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "                    q = self.policy_net(s)\n",
    "                    action = int(q.argmax(dim=1).item())\n",
    "                self.policy_net.train()\n",
    "            else:\n",
    "                action = self.env.action_space.sample()\n",
    "        else:\n",
    "            self.policy_net.eval()\n",
    "            with torch.no_grad():\n",
    "                s = torch.as_tensor(state, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "                q = self.policy_net(s)\n",
    "                action = int(q.argmax(dim=1).item())\n",
    "            self.policy_net.train()\n",
    "        return action\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.replay) < MIN_REPLAY_SIZE:\n",
    "            return None\n",
    "        states, actions, rewards, next_states, dones = self.replay.sample(BATCH_SIZE)\n",
    "\n",
    "        # Double DQN target computation\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.policy_net(next_states).argmax(dim=1, keepdim=True)\n",
    "            next_q = self.target_net(next_states).gather(1, next_actions)\n",
    "            target_q = rewards + (1.0 - dones) * GAMMA * next_q\n",
    "\n",
    "        current_q = self.policy_net(states).gather(1, actions)\n",
    "        loss = self.loss_fn(current_q, target_q)\n",
    "\n",
    "        # optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.steps_done += 1\n",
    "        # hard target update\n",
    "        if self.steps_done % TARGET_UPDATE_FREQ == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self, num_episodes=EPISODES_PER_SEED):\n",
    "        episode_rewards = []\n",
    "        total_steps = 0\n",
    "        for ep in range(1, num_episodes + 1):\n",
    "            obs, _ = self.env.reset()\n",
    "            state = decode_state_index(int(obs), self.env)\n",
    "            ep_reward = 0.0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.select_action(state, explore=True)\n",
    "                step_ret = self.env.step(action)\n",
    "                # support both gym and custom five-tuple\n",
    "                if len(step_ret) == 5:\n",
    "                    ns_raw, r, terminated, truncated, info = step_ret\n",
    "                    done = bool(terminated or truncated)\n",
    "                else:\n",
    "                    ns_raw, r, done, info = step_ret\n",
    "                next_state = decode_state_index(int(ns_raw), self.env)\n",
    "\n",
    "                # push to replay\n",
    "                self.replay.push(state, action, float(r), next_state, float(done))\n",
    "                state = next_state\n",
    "                ep_reward += float(r)\n",
    "                total_steps += 1\n",
    "\n",
    "                # update\n",
    "                _loss = self.optimize_model()\n",
    "\n",
    "            # decay epsilon per episode (multiplicative)\n",
    "            self.epsilon = max(EPS_END, self.epsilon * EPS_DECAY)\n",
    "            episode_rewards.append(ep_reward)\n",
    "\n",
    "            # optionally print\n",
    "            if ep % 50 == 0:\n",
    "                recent = np.mean(episode_rewards[-50:])\n",
    "                print(f\"Seed {self.seed} Ep {ep}/{num_episodes}  recent_avg={recent:.2f}  eps={self.epsilon:.3f}  replay={len(self.replay)}\")\n",
    "\n",
    "        return episode_rewards\n",
    "\n",
    "    def evaluate(self, num_episodes=100):\n",
    "        rewards = []\n",
    "        for _ in range(num_episodes):\n",
    "            obs, _ = self.env.reset()\n",
    "            state = decode_state_index(int(obs), self.env)\n",
    "            done = False\n",
    "            ep_r = 0.0\n",
    "            while not done:\n",
    "                action = self.select_action(state, explore=False)\n",
    "                step_ret = self.env.step(action)\n",
    "                if len(step_ret) == 5:\n",
    "                    ns_raw, r, terminated, truncated, info = step_ret\n",
    "                    done = bool(terminated or truncated)\n",
    "                else:\n",
    "                    ns_raw, r, done, info = step_ret\n",
    "                state = decode_state_index(int(ns_raw), self.env)\n",
    "                ep_r += float(r)\n",
    "            rewards.append(ep_r)\n",
    "        return float(np.mean(rewards)), float(np.std(rewards))\n",
    "\n",
    "\n",
    "# ------------------------------ Main -----------------------------------\n",
    "\n",
    "def run_multi_seed_training(env, network_class, model_name_prefix):\n",
    "    all_seed_rewards = []\n",
    "    best_model_path = os.path.join(MODEL_DIR, model_name_prefix)\n",
    "    best_avg = -1e9\n",
    "    best_rewards_history = None\n",
    "\n",
    "    for seed in range(NUM_SEEDS):\n",
    "        print(f\"\\nStarting seed {seed} for {model_name_prefix}\")\n",
    "        set_seed(seed)\n",
    "        trainer = DQNTrainer(env, network_class, lr=LR, seed=seed)\n",
    "        rewards = trainer.train(num_episodes=EPISODES_PER_SEED)\n",
    "\n",
    "        avg_last100 = float(np.mean(rewards[-100:]))\n",
    "        print(f\"Seed {seed} finished. avg_last100={avg_last100:.2f}\")\n",
    "\n",
    "        # Save best model (by last-100 average)\n",
    "        if avg_last100 > best_avg:\n",
    "            best_avg = avg_last100\n",
    "            best_rewards_history = rewards\n",
    "            path = os.path.join(MODEL_DIR, model_name_prefix)\n",
    "            torch.save(trainer.policy_net.state_dict(), path)\n",
    "            print(f\"Saved new best model to {path} (avg_last100={best_avg:.2f})\")\n",
    "\n",
    "        all_seed_rewards.append(rewards)\n",
    "\n",
    "    # compute mean reward curve across seeds (pad to same length)\n",
    "    maxlen = max(len(r) for r in all_seed_rewards)\n",
    "    padded = [r + [r[-1]] * (maxlen - len(r)) for r in all_seed_rewards]\n",
    "    mean_curve = np.mean(np.array(padded), axis=0)\n",
    "    return mean_curve, best_model_path, best_rewards_history\n",
    "\n",
    "\n",
    "def main():\n",
    "    # set global seed for reproducibility\n",
    "    GLOBAL_SEED = 509\n",
    "    set_seed(GLOBAL_SEED)\n",
    "\n",
    "    train_env = MultiGoalCliffWalkingEnv(train=True)\n",
    "    eval_env = MultiGoalCliffWalkingEnv(train=False)\n",
    "\n",
    "    # Train linear agent\n",
    "    print(\"Training Linear Agent...\")\n",
    "    linear_mean_curve, linear_model_path, linear_history = run_multi_seed_training(train_env, LinearDQN, \"best linear.pt\")\n",
    "\n",
    "    # Train non-linear agent\n",
    "    print(\"Training Non-Linear Agent...\")\n",
    "    nonlinear_mean_curve, nonlinear_model_path, nonlinear_history = run_multi_seed_training(train_env, NonLinearDQN, \"best nonlinear.pt\")\n",
    "\n",
    "    # Plot and save training reward curves (moving average 50)\n",
    "    def save_plot(curve, filename, title):\n",
    "        window = 50\n",
    "        movavg = np.convolve(curve, np.ones(window)/window, mode='valid')\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.plot(movavg)\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Episode (smoothed)')\n",
    "        plt.ylabel('Average reward')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "\n",
    "    save_plot(linear_mean_curve, os.path.join(PLOT_DIR, \"cliff average rewards linear.png\"), \"Cliff - Linear DQN Average Rewards\")\n",
    "    save_plot(nonlinear_mean_curve, os.path.join(PLOT_DIR, \"cliff average rewards nonlinear.png\"), \"Cliff - Nonlinear DQN Average Rewards\")\n",
    "\n",
    "    # Evaluation using best saved models (100 episodes each)\n",
    "    print(\"Evaluation...\")\n",
    "    results = {}\n",
    "\n",
    "    # Linear eval\n",
    "    linear_model = LinearDQN(4, train_env.action_space.n).to(DEVICE)\n",
    "    linear_model.load_state_dict(torch.load(os.path.join(MODEL_DIR, \"best linear.pt\"), map_location=DEVICE))\n",
    "    linear_trainer = DQNTrainer(eval_env, LinearDQN, lr=LR)\n",
    "    linear_trainer.policy_net = linear_model\n",
    "    mean_l, std_l = linear_trainer.evaluate(num_episodes=100)\n",
    "    results['linear'] = {'mean': mean_l, 'std': std_l}\n",
    "\n",
    "    # Nonlinear eval\n",
    "    nonlinear_model = NonLinearDQN(4, train_env.action_space.n).to(DEVICE)\n",
    "    nonlinear_model.load_state_dict(torch.load(os.path.join(MODEL_DIR, \"best nonlinear.pt\"), map_location=DEVICE))\n",
    "    nonlinear_trainer = DQNTrainer(eval_env, NonLinearDQN, lr=LR)\n",
    "    nonlinear_trainer.policy_net = nonlinear_model\n",
    "    mean_n, std_n = nonlinear_trainer.evaluate(num_episodes=100)\n",
    "    results['nonlinear'] = {'mean': mean_n, 'std': std_n}\n",
    "\n",
    "    # Save JSON\n",
    "    json_path = os.path.join(EVAL_DIR, \"cliff evaluation results.json\")\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    print(\"Done. Models, plots, and evaluation results saved under Q1/\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
