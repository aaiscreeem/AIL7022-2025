{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06ea99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Agent Initialized (1-Step DQN) ---\n",
      "Device: cuda\n",
      "Engineered State Dim: 27\n",
      "\n",
      "--- Starting Training ---\n",
      "Episode 100\tAvg Score (100): -14.27\tEpsilon: 0.990\n",
      "Episode 200\tAvg Score (100): -14.41\tEpsilon: 0.980\n",
      "Episode 300\tAvg Score (100): -13.53\tEpsilon: 0.966\n",
      "Episode 400\tAvg Score (100): -14.55\tEpsilon: 0.956\n",
      "Episode 500\tAvg Score (100): -17.33\tEpsilon: 0.942\n",
      "Episode 600\tAvg Score (100): -15.63\tEpsilon: 0.932\n",
      "Episode 700\tAvg Score (100): -14.28\tEpsilon: 0.918\n",
      "Episode 800\tAvg Score (100): -14.01\tEpsilon: 0.909\n",
      "Episode 900\tAvg Score (100): -14.21\tEpsilon: 0.896\n",
      "Episode 1000\tAvg Score (100): -14.21\tEpsilon: 0.887\n",
      "Episode 1100\tAvg Score (100): -14.11\tEpsilon: 0.873\n",
      "Episode 1200\tAvg Score (100): -14.29\tEpsilon: 0.865\n",
      "Episode 1300\tAvg Score (100): -13.81\tEpsilon: 0.852\n",
      "Episode 1400\tAvg Score (100): -14.52\tEpsilon: 0.843\n",
      "Episode 1500\tAvg Score (100): -13.95\tEpsilon: 0.831\n",
      "Episode 1600\tAvg Score (100): -16.86\tEpsilon: 0.822\n",
      "Episode 1700\tAvg Score (100): -15.21\tEpsilon: 0.810\n",
      "Episode 1800\tAvg Score (100): -14.65\tEpsilon: 0.802\n",
      "Episode 1900\tAvg Score (100): -14.85\tEpsilon: 0.790\n",
      "Episode 2000\tAvg Score (100): -16.52\tEpsilon: 0.782\n",
      "Episode 2100\tAvg Score (100): -14.01\tEpsilon: 0.771\n",
      "Episode 2200\tAvg Score (100): -15.85\tEpsilon: 0.763\n",
      "Episode 2300\tAvg Score (100): -14.46\tEpsilon: 0.751\n",
      "Episode 2400\tAvg Score (100): -13.83\tEpsilon: 0.744\n",
      "Episode 2500\tAvg Score (100): -14.10\tEpsilon: 0.733\n",
      "Episode 2600\tAvg Score (100): -17.13\tEpsilon: 0.726\n",
      "Episode 2700\tAvg Score (100): -13.82\tEpsilon: 0.715\n",
      "Episode 2800\tAvg Score (100): -13.15\tEpsilon: 0.708\n",
      "Episode 2900\tAvg Score (100): -13.55\tEpsilon: 0.697\n",
      "Episode 3000\tAvg Score (100): -15.34\tEpsilon: 0.690\n",
      "Episode 3100\tAvg Score (100): -13.58\tEpsilon: 0.680\n",
      "Episode 3200\tAvg Score (100): -13.92\tEpsilon: 0.673\n",
      "Episode 3300\tAvg Score (100): -15.96\tEpsilon: 0.663\n",
      "Episode 3400\tAvg Score (100): -16.37\tEpsilon: 0.656\n",
      "Episode 3500\tAvg Score (100): -14.75\tEpsilon: 0.647\n",
      "Episode 3600\tAvg Score (100): -12.62\tEpsilon: 0.640\n",
      "Episode 3700\tAvg Score (100): -14.00\tEpsilon: 0.631\n",
      "Episode 3800\tAvg Score (100): -12.32\tEpsilon: 0.624\n",
      "Episode 3900\tAvg Score (100): -12.78\tEpsilon: 0.615\n",
      "Episode 4000\tAvg Score (100): -12.81\tEpsilon: 0.609\n",
      "Episode 4100\tAvg Score (100): -13.91\tEpsilon: 0.600\n",
      "Episode 4200\tAvg Score (100): -10.78\tEpsilon: 0.594\n",
      "Episode 4300\tAvg Score (100): -11.16\tEpsilon: 0.585\n",
      "Episode 4400\tAvg Score (100): -10.77\tEpsilon: 0.579\n",
      "Episode 4500\tAvg Score (100): -13.88\tEpsilon: 0.570\n",
      "Episode 4600\tAvg Score (100): -9.73\tEpsilon: 0.565\n",
      "Episode 4700\tAvg Score (100): -10.64\tEpsilon: 0.556\n",
      "Episode 4800\tAvg Score (100): -7.82\tEpsilon: 0.551\n",
      "Episode 4900\tAvg Score (100): -10.33\tEpsilon: 0.543\n",
      "Episode 5000\tAvg Score (100): -6.92\tEpsilon: 0.537\n",
      "Episode 5100\tAvg Score (100): -7.91\tEpsilon: 0.529\n",
      "Episode 5200\tAvg Score (100): -7.82\tEpsilon: 0.524\n",
      "Episode 5300\tAvg Score (100): -8.37\tEpsilon: 0.516\n",
      "Episode 5400\tAvg Score (100): -8.91\tEpsilon: 0.511\n",
      "Episode 5500\tAvg Score (100): -6.89\tEpsilon: 0.503\n",
      "Episode 5600\tAvg Score (100): -7.52\tEpsilon: 0.498\n",
      "Episode 5700\tAvg Score (100): -9.40\tEpsilon: 0.491\n",
      "Episode 5800\tAvg Score (100): -6.47\tEpsilon: 0.486\n",
      "Episode 5900\tAvg Score (100): -5.91\tEpsilon: 0.479\n",
      "Episode 6000\tAvg Score (100): -5.31\tEpsilon: 0.474\n",
      "Episode 6100\tAvg Score (100): -4.58\tEpsilon: 0.467\n",
      "Episode 6200\tAvg Score (100): -4.48\tEpsilon: 0.462\n",
      "Episode 6300\tAvg Score (100): -6.10\tEpsilon: 0.455\n",
      "Episode 6400\tAvg Score (100): -5.18\tEpsilon: 0.451\n",
      "Episode 6500\tAvg Score (100): -7.94\tEpsilon: 0.444\n",
      "Episode 6600\tAvg Score (100): -7.13\tEpsilon: 0.440\n",
      "Episode 6700\tAvg Score (100): -5.36\tEpsilon: 0.433\n",
      "Episode 6800\tAvg Score (100): -5.77\tEpsilon: 0.429\n",
      "Episode 6900\tAvg Score (100): -8.24\tEpsilon: 0.422\n",
      "Episode 7000\tAvg Score (100): -7.70\tEpsilon: 0.418\n",
      "Episode 7100\tAvg Score (100): -5.39\tEpsilon: 0.412\n",
      "Episode 7200\tAvg Score (100): -4.42\tEpsilon: 0.408\n",
      "Episode 7300\tAvg Score (100): -2.60\tEpsilon: 0.402\n",
      "Episode 7400\tAvg Score (100): -3.73\tEpsilon: 0.398\n",
      "Episode 7500\tAvg Score (100): -4.43\tEpsilon: 0.392\n",
      "Episode 7600\tAvg Score (100): -3.98\tEpsilon: 0.388\n",
      "Episode 7700\tAvg Score (100): -4.89\tEpsilon: 0.382\n",
      "Episode 7800\tAvg Score (100): -3.02\tEpsilon: 0.378\n",
      "Episode 7900\tAvg Score (100): -3.82\tEpsilon: 0.373\n",
      "Episode 8000\tAvg Score (100): -2.21\tEpsilon: 0.369\n",
      "Episode 8100\tAvg Score (100): -3.45\tEpsilon: 0.363\n",
      "Episode 8200\tAvg Score (100): -2.15\tEpsilon: 0.360\n",
      "Episode 8300\tAvg Score (100): -2.59\tEpsilon: 0.354\n",
      "Episode 8400\tAvg Score (100): -2.73\tEpsilon: 0.351\n",
      "Episode 8500\tAvg Score (100): -4.81\tEpsilon: 0.346\n",
      "Episode 8600\tAvg Score (100): -3.00\tEpsilon: 0.342\n",
      "Episode 8700\tAvg Score (100): -0.57\tEpsilon: 0.337\n",
      "Episode 8800\tAvg Score (100): -2.40\tEpsilon: 0.334\n",
      "Episode 8900\tAvg Score (100): -1.20\tEpsilon: 0.329\n",
      "Episode 9000\tAvg Score (100): -2.61\tEpsilon: 0.325\n",
      "Episode 9100\tAvg Score (100): -0.55\tEpsilon: 0.321\n",
      "Episode 9200\tAvg Score (100): -4.09\tEpsilon: 0.317\n",
      "Episode 9300\tAvg Score (100): 0.93\tEpsilon: 0.313\n",
      "Episode 9400\tAvg Score (100): -1.67\tEpsilon: 0.309\n",
      "Episode 9500\tAvg Score (100): 0.57\tEpsilon: 0.305\n",
      "Episode 9600\tAvg Score (100): -2.45\tEpsilon: 0.302\n",
      "Episode 9700\tAvg Score (100): -1.26\tEpsilon: 0.297\n",
      "Episode 9800\tAvg Score (100): -1.47\tEpsilon: 0.294\n",
      "Episode 9900\tAvg Score (100): -1.79\tEpsilon: 0.290\n",
      "Episode 10000\tAvg Score (100): -3.90\tEpsilon: 0.287\n",
      "Episode 10100\tAvg Score (100): -1.28\tEpsilon: 0.283\n",
      "Episode 10200\tAvg Score (100): -4.30\tEpsilon: 0.280\n",
      "Episode 10300\tAvg Score (100): -1.40\tEpsilon: 0.276\n",
      "Episode 10400\tAvg Score (100): -2.74\tEpsilon: 0.273\n",
      "Episode 10500\tAvg Score (100): -4.86\tEpsilon: 0.269\n",
      "Episode 10600\tAvg Score (100): -3.36\tEpsilon: 0.266\n",
      "Episode 10700\tAvg Score (100): -0.81\tEpsilon: 0.262\n",
      "Episode 10800\tAvg Score (100): -0.86\tEpsilon: 0.260\n",
      "Episode 10900\tAvg Score (100): 1.18\tEpsilon: 0.256\n",
      "Episode 11000\tAvg Score (100): -1.26\tEpsilon: 0.253\n",
      "Episode 11100\tAvg Score (100): 0.15\tEpsilon: 0.249\n",
      "Episode 11200\tAvg Score (100): -2.37\tEpsilon: 0.247\n",
      "Episode 11300\tAvg Score (100): -1.32\tEpsilon: 0.243\n",
      "Episode 11400\tAvg Score (100): 0.15\tEpsilon: 0.241\n",
      "Episode 11500\tAvg Score (100): -0.24\tEpsilon: 0.237\n",
      "Episode 11600\tAvg Score (100): -1.24\tEpsilon: 0.235\n",
      "Episode 11700\tAvg Score (100): -2.27\tEpsilon: 0.231\n",
      "Episode 11800\tAvg Score (100): -1.57\tEpsilon: 0.229\n",
      "Episode 11900\tAvg Score (100): 2.48\tEpsilon: 0.226\n",
      "Episode 12000\tAvg Score (100): -2.13\tEpsilon: 0.223\n",
      "Episode 12100\tAvg Score (100): -0.07\tEpsilon: 0.220\n",
      "Episode 12200\tAvg Score (100): 0.66\tEpsilon: 0.218\n",
      "Episode 12300\tAvg Score (100): 0.29\tEpsilon: 0.215\n",
      "Episode 12400\tAvg Score (100): 2.80\tEpsilon: 0.212\n",
      "Episode 12500\tAvg Score (100): -1.75\tEpsilon: 0.209\n",
      "Episode 12600\tAvg Score (100): 0.62\tEpsilon: 0.207\n",
      "Episode 12700\tAvg Score (100): 2.04\tEpsilon: 0.204\n",
      "Episode 12800\tAvg Score (100): 0.26\tEpsilon: 0.202\n",
      "Episode 12900\tAvg Score (100): 2.09\tEpsilon: 0.199\n",
      "Episode 13000\tAvg Score (100): 2.45\tEpsilon: 0.197\n",
      "Episode 13100\tAvg Score (100): 1.44\tEpsilon: 0.194\n",
      "Episode 13200\tAvg Score (100): 0.85\tEpsilon: 0.192\n",
      "Episode 13300\tAvg Score (100): 0.69\tEpsilon: 0.189\n",
      "Episode 13400\tAvg Score (100): 1.14\tEpsilon: 0.187\n",
      "Episode 13500\tAvg Score (100): 5.41\tEpsilon: 0.185\n",
      "Episode 13600\tAvg Score (100): 3.52\tEpsilon: 0.183\n",
      "Episode 13700\tAvg Score (100): 3.23\tEpsilon: 0.180\n",
      "Episode 13800\tAvg Score (100): 1.69\tEpsilon: 0.178\n",
      "Episode 13900\tAvg Score (100): 4.24\tEpsilon: 0.176\n",
      "Episode 14000\tAvg Score (100): 4.69\tEpsilon: 0.174\n",
      "Episode 14100\tAvg Score (100): 4.22\tEpsilon: 0.171\n",
      "Episode 14200\tAvg Score (100): 4.58\tEpsilon: 0.170\n",
      "Episode 14300\tAvg Score (100): 4.29\tEpsilon: 0.167\n",
      "Episode 14400\tAvg Score (100): 2.57\tEpsilon: 0.165\n",
      "Episode 14500\tAvg Score (100): 2.08\tEpsilon: 0.163\n",
      "Episode 14600\tAvg Score (100): 2.17\tEpsilon: 0.161\n",
      "Episode 14700\tAvg Score (100): 1.35\tEpsilon: 0.159\n",
      "Episode 14800\tAvg Score (100): 5.75\tEpsilon: 0.157\n",
      "Episode 14900\tAvg Score (100): 0.55\tEpsilon: 0.155\n",
      "Episode 15000\tAvg Score (100): 3.21\tEpsilon: 0.153\n",
      "Episode 15100\tAvg Score (100): 2.69\tEpsilon: 0.151\n",
      "Episode 15200\tAvg Score (100): 4.05\tEpsilon: 0.150\n",
      "Episode 15300\tAvg Score (100): 3.77\tEpsilon: 0.147\n",
      "Episode 15400\tAvg Score (100): 0.23\tEpsilon: 0.146\n",
      "Episode 15500\tAvg Score (100): 3.40\tEpsilon: 0.144\n",
      "Episode 15600\tAvg Score (100): 5.28\tEpsilon: 0.142\n",
      "Episode 15700\tAvg Score (100): 2.48\tEpsilon: 0.140\n",
      "Episode 15800\tAvg Score (100): 2.35\tEpsilon: 0.139\n",
      "Episode 15900\tAvg Score (100): 4.06\tEpsilon: 0.137\n",
      "Episode 16000\tAvg Score (100): 1.57\tEpsilon: 0.135\n",
      "Episode 16100\tAvg Score (100): 3.79\tEpsilon: 0.133\n",
      "Episode 16200\tAvg Score (100): 3.70\tEpsilon: 0.132\n",
      "Episode 16300\tAvg Score (100): 2.47\tEpsilon: 0.130\n",
      "Episode 16400\tAvg Score (100): 6.54\tEpsilon: 0.129\n",
      "Episode 16500\tAvg Score (100): 6.20\tEpsilon: 0.127\n",
      "Episode 16600\tAvg Score (100): 2.44\tEpsilon: 0.126\n",
      "Episode 16700\tAvg Score (100): 4.86\tEpsilon: 0.124\n",
      "Episode 16800\tAvg Score (100): 5.39\tEpsilon: 0.122\n",
      "Episode 16900\tAvg Score (100): 3.06\tEpsilon: 0.121\n",
      "Episode 17000\tAvg Score (100): 3.28\tEpsilon: 0.119\n",
      "Episode 17100\tAvg Score (100): 3.03\tEpsilon: 0.118\n",
      "Episode 17200\tAvg Score (100): 3.52\tEpsilon: 0.116\n",
      "Episode 17300\tAvg Score (100): 5.84\tEpsilon: 0.115\n",
      "Episode 17400\tAvg Score (100): 7.20\tEpsilon: 0.114\n",
      "Episode 17500\tAvg Score (100): 4.25\tEpsilon: 0.112\n",
      "Episode 17600\tAvg Score (100): 3.80\tEpsilon: 0.111\n",
      "Episode 17700\tAvg Score (100): 7.38\tEpsilon: 0.109\n",
      "Episode 17800\tAvg Score (100): 4.99\tEpsilon: 0.108\n",
      "Episode 17900\tAvg Score (100): 5.40\tEpsilon: 0.106\n",
      "Episode 18000\tAvg Score (100): 5.50\tEpsilon: 0.105\n",
      "Episode 18100\tAvg Score (100): 2.96\tEpsilon: 0.104\n",
      "Episode 18200\tAvg Score (100): 5.17\tEpsilon: 0.103\n",
      "Episode 18300\tAvg Score (100): 7.12\tEpsilon: 0.101\n",
      "Episode 18400\tAvg Score (100): 4.83\tEpsilon: 0.100\n",
      "Episode 18500\tAvg Score (100): 6.79\tEpsilon: 0.100\n",
      "Episode 18600\tAvg Score (100): 5.29\tEpsilon: 0.100\n",
      "Episode 18700\tAvg Score (100): 5.31\tEpsilon: 0.100\n",
      "Episode 18800\tAvg Score (100): 7.44\tEpsilon: 0.100\n",
      "Episode 18900\tAvg Score (100): 8.15\tEpsilon: 0.100\n",
      "Episode 19000\tAvg Score (100): 8.34\tEpsilon: 0.100\n",
      "Episode 19100\tAvg Score (100): 2.01\tEpsilon: 0.100\n",
      "Episode 19200\tAvg Score (100): 5.73\tEpsilon: 0.100\n",
      "Episode 19300\tAvg Score (100): 4.91\tEpsilon: 0.100\n",
      "Episode 19400\tAvg Score (100): 2.34\tEpsilon: 0.100\n",
      "Episode 19500\tAvg Score (100): 8.53\tEpsilon: 0.100\n",
      "Episode 19600\tAvg Score (100): 4.07\tEpsilon: 0.100\n",
      "Episode 19700\tAvg Score (100): 3.92\tEpsilon: 0.100\n",
      "Episode 19800\tAvg Score (100): 4.03\tEpsilon: 0.100\n",
      "Episode 19900\tAvg Score (100): 4.92\tEpsilon: 0.100\n",
      "Episode 20000\tAvg Score (100): 5.89\tEpsilon: 0.100\n",
      "Episode 20100\tAvg Score (100): 5.40\tEpsilon: 0.100\n",
      "Episode 20200\tAvg Score (100): 4.14\tEpsilon: 0.100\n",
      "Episode 20300\tAvg Score (100): 3.25\tEpsilon: 0.100\n",
      "Episode 20400\tAvg Score (100): 6.08\tEpsilon: 0.100\n",
      "Episode 20500\tAvg Score (100): 3.32\tEpsilon: 0.100\n",
      "Episode 20600\tAvg Score (100): 4.26\tEpsilon: 0.100\n",
      "Episode 20700\tAvg Score (100): 4.92\tEpsilon: 0.100\n",
      "Episode 20800\tAvg Score (100): 6.77\tEpsilon: 0.100\n",
      "Episode 20900\tAvg Score (100): 6.12\tEpsilon: 0.100\n",
      "Episode 21000\tAvg Score (100): 6.31\tEpsilon: 0.100\n",
      "Episode 21100\tAvg Score (100): 6.62\tEpsilon: 0.100\n",
      "Episode 21200\tAvg Score (100): 5.86\tEpsilon: 0.100\n",
      "Episode 21300\tAvg Score (100): 8.58\tEpsilon: 0.100\n",
      "Episode 21400\tAvg Score (100): 7.67\tEpsilon: 0.100\n",
      "Episode 21500\tAvg Score (100): 3.40\tEpsilon: 0.100\n",
      "Episode 21600\tAvg Score (100): 2.73\tEpsilon: 0.100\n",
      "Episode 21700\tAvg Score (100): 3.63\tEpsilon: 0.100\n",
      "Episode 21800\tAvg Score (100): 5.94\tEpsilon: 0.100\n",
      "Episode 21900\tAvg Score (100): 7.14\tEpsilon: 0.100\n",
      "Episode 22000\tAvg Score (100): 6.94\tEpsilon: 0.100\n",
      "Episode 22100\tAvg Score (100): 7.87\tEpsilon: 0.100\n",
      "Episode 22200\tAvg Score (100): 7.10\tEpsilon: 0.100\n",
      "Episode 22300\tAvg Score (100): 6.48\tEpsilon: 0.100\n",
      "Episode 22400\tAvg Score (100): 2.54\tEpsilon: 0.100\n",
      "Episode 22500\tAvg Score (100): 11.20\tEpsilon: 0.100\n",
      "\n",
      "--- Environment Good in 22500 episodes! ---\n",
      "Episode 22600\tAvg Score (100): 6.41\tEpsilon: 0.100\n",
      "Episode 22700\tAvg Score (100): 6.25\tEpsilon: 0.100\n",
      "Episode 22800\tAvg Score (100): 6.73\tEpsilon: 0.100\n",
      "Episode 22900\tAvg Score (100): 8.16\tEpsilon: 0.100\n",
      "Episode 23000\tAvg Score (100): 6.63\tEpsilon: 0.100\n",
      "Episode 23100\tAvg Score (100): 6.58\tEpsilon: 0.100\n",
      "Episode 23200\tAvg Score (100): 10.18\tEpsilon: 0.100\n",
      "\n",
      "--- Environment Good in 23200 episodes! ---\n",
      "Episode 23300\tAvg Score (100): 4.27\tEpsilon: 0.100\n",
      "Episode 23400\tAvg Score (100): 8.83\tEpsilon: 0.100\n",
      "Episode 23500\tAvg Score (100): 6.58\tEpsilon: 0.100\n",
      "Episode 23600\tAvg Score (100): 6.07\tEpsilon: 0.100\n",
      "Episode 23700\tAvg Score (100): 6.54\tEpsilon: 0.100\n",
      "Episode 23800\tAvg Score (100): 7.87\tEpsilon: 0.100\n",
      "Episode 23900\tAvg Score (100): 7.88\tEpsilon: 0.100\n",
      "Episode 24000\tAvg Score (100): 9.14\tEpsilon: 0.100\n",
      "Episode 24100\tAvg Score (100): 8.14\tEpsilon: 0.100\n",
      "Episode 24200\tAvg Score (100): 5.23\tEpsilon: 0.100\n",
      "Episode 24300\tAvg Score (100): 9.47\tEpsilon: 0.100\n",
      "Episode 24400\tAvg Score (100): 6.21\tEpsilon: 0.100\n",
      "Episode 24500\tAvg Score (100): 5.92\tEpsilon: 0.100\n",
      "Episode 24600\tAvg Score (100): 9.88\tEpsilon: 0.100\n",
      "Episode 24700\tAvg Score (100): 7.30\tEpsilon: 0.100\n",
      "Episode 24800\tAvg Score (100): 5.06\tEpsilon: 0.100\n",
      "Episode 24900\tAvg Score (100): 9.69\tEpsilon: 0.100\n",
      "Episode 25000\tAvg Score (100): 4.92\tEpsilon: 0.100\n",
      "Episode 25100\tAvg Score (100): 7.54\tEpsilon: 0.100\n",
      "Episode 25200\tAvg Score (100): 8.79\tEpsilon: 0.100\n",
      "Episode 25300\tAvg Score (100): 4.49\tEpsilon: 0.100\n",
      "Episode 25400\tAvg Score (100): 5.04\tEpsilon: 0.100\n",
      "Episode 25500\tAvg Score (100): 9.93\tEpsilon: 0.100\n",
      "Episode 25600\tAvg Score (100): 7.57\tEpsilon: 0.100\n",
      "Episode 25700\tAvg Score (100): 9.15\tEpsilon: 0.100\n",
      "Episode 25800\tAvg Score (100): 7.76\tEpsilon: 0.100\n",
      "Episode 25900\tAvg Score (100): 7.00\tEpsilon: 0.100\n",
      "Episode 26000\tAvg Score (100): 9.15\tEpsilon: 0.100\n",
      "Episode 26100\tAvg Score (100): 9.06\tEpsilon: 0.100\n",
      "Episode 26200\tAvg Score (100): 9.31\tEpsilon: 0.100\n",
      "Episode 26300\tAvg Score (100): 6.38\tEpsilon: 0.100\n",
      "Episode 26400\tAvg Score (100): 7.64\tEpsilon: 0.100\n",
      "Episode 26500\tAvg Score (100): 7.29\tEpsilon: 0.100\n",
      "Episode 26600\tAvg Score (100): 7.96\tEpsilon: 0.100\n",
      "Episode 26700\tAvg Score (100): 9.24\tEpsilon: 0.100\n",
      "Episode 26800\tAvg Score (100): 7.54\tEpsilon: 0.100\n",
      "Episode 26900\tAvg Score (100): 6.62\tEpsilon: 0.100\n",
      "Episode 27000\tAvg Score (100): 3.10\tEpsilon: 0.100\n",
      "Episode 27100\tAvg Score (100): 8.24\tEpsilon: 0.100\n",
      "Episode 27200\tAvg Score (100): 7.39\tEpsilon: 0.100\n",
      "Episode 27300\tAvg Score (100): 5.85\tEpsilon: 0.100\n",
      "Episode 27400\tAvg Score (100): 9.15\tEpsilon: 0.100\n",
      "Episode 27500\tAvg Score (100): 8.69\tEpsilon: 0.100\n",
      "Episode 27600\tAvg Score (100): 7.52\tEpsilon: 0.100\n",
      "Episode 27700\tAvg Score (100): 8.96\tEpsilon: 0.100\n",
      "Episode 27800\tAvg Score (100): 8.29\tEpsilon: 0.100\n",
      "Episode 27900\tAvg Score (100): 6.69\tEpsilon: 0.100\n",
      "Episode 28000\tAvg Score (100): 6.32\tEpsilon: 0.100\n",
      "Episode 28100\tAvg Score (100): 9.79\tEpsilon: 0.100\n",
      "Episode 28200\tAvg Score (100): 9.17\tEpsilon: 0.100\n",
      "Episode 28300\tAvg Score (100): 8.13\tEpsilon: 0.100\n",
      "Episode 28400\tAvg Score (100): 8.16\tEpsilon: 0.100\n",
      "Episode 28500\tAvg Score (100): 8.36\tEpsilon: 0.100\n",
      "Episode 28600\tAvg Score (100): 10.62\tEpsilon: 0.100\n",
      "\n",
      "--- Environment Good in 28600 episodes! ---\n",
      "Episode 28700\tAvg Score (100): 10.11\tEpsilon: 0.100\n",
      "\n",
      "--- Environment Good in 28700 episodes! ---\n",
      "Episode 28800\tAvg Score (100): 6.59\tEpsilon: 0.100\n",
      "Episode 28900\tAvg Score (100): 6.12\tEpsilon: 0.100\n",
      "Episode 29000\tAvg Score (100): 10.56\tEpsilon: 0.100\n",
      "\n",
      "--- Environment Good in 29000 episodes! ---\n",
      "Episode 29100\tAvg Score (100): 6.84\tEpsilon: 0.100\n",
      "Episode 29200\tAvg Score (100): 9.68\tEpsilon: 0.100\n",
      "Episode 29300\tAvg Score (100): 3.43\tEpsilon: 0.100\n",
      "Episode 29400\tAvg Score (100): 9.99\tEpsilon: 0.100\n",
      "Episode 29500\tAvg Score (100): 7.17\tEpsilon: 0.100\n",
      "Episode 29600\tAvg Score (100): 6.21\tEpsilon: 0.100\n",
      "Episode 29700\tAvg Score (100): 10.93\tEpsilon: 0.100\n",
      "\n",
      "--- Environment Good in 29700 episodes! ---\n",
      "Episode 29800\tAvg Score (100): 9.50\tEpsilon: 0.100\n",
      "Episode 29900\tAvg Score (100): 7.31\tEpsilon: 0.100\n",
      "Episode 30000\tAvg Score (100): 9.94\tEpsilon: 0.100\n",
      "Episode 30100\tAvg Score (100): 7.64\tEpsilon: 0.100\n",
      "Episode 30200\tAvg Score (100): 10.47\tEpsilon: 0.100\n",
      "\n",
      "--- Environment Good in 30200 episodes! ---\n",
      "Episode 30300\tAvg Score (100): 6.12\tEpsilon: 0.100\n",
      "Episode 30400\tAvg Score (100): 8.54\tEpsilon: 0.100\n",
      "Episode 30500\tAvg Score (100): 8.31\tEpsilon: 0.100\n",
      "Episode 30600\tAvg Score (100): 8.95\tEpsilon: 0.100\n",
      "Episode 30700\tAvg Score (100): 7.72\tEpsilon: 0.100\n",
      "Episode 30800\tAvg Score (100): 9.56\tEpsilon: 0.100\n",
      "Episode 30900\tAvg Score (100): 8.26\tEpsilon: 0.100\n",
      "Episode 31000\tAvg Score (100): 9.33\tEpsilon: 0.100\n",
      "Episode 31100\tAvg Score (100): 8.35\tEpsilon: 0.100\n",
      "Episode 31200\tAvg Score (100): 8.36\tEpsilon: 0.100\n",
      "Episode 31300\tAvg Score (100): 7.49\tEpsilon: 0.100\n",
      "Episode 31400\tAvg Score (100): 5.73\tEpsilon: 0.100\n",
      "Episode 31500\tAvg Score (100): 7.41\tEpsilon: 0.100\n",
      "Episode 31600\tAvg Score (100): 7.82\tEpsilon: 0.100\n",
      "Episode 31700\tAvg Score (100): 12.60\tEpsilon: 0.100\n",
      "\n",
      "--- Environment Good in 31700 episodes! ---\n",
      "Episode 31800\tAvg Score (100): 6.72\tEpsilon: 0.100\n",
      "Episode 31900\tAvg Score (100): 8.36\tEpsilon: 0.100\n",
      "Episode 32000\tAvg Score (100): 7.87\tEpsilon: 0.100\n",
      "Episode 32100\tAvg Score (100): 8.95\tEpsilon: 0.100\n",
      "Episode 32200\tAvg Score (100): 8.42\tEpsilon: 0.100\n",
      "Episode 32300\tAvg Score (100): 9.16\tEpsilon: 0.100\n",
      "Episode 32400\tAvg Score (100): 8.11\tEpsilon: 0.100\n",
      "Episode 32500\tAvg Score (100): 8.13\tEpsilon: 0.100\n",
      "Episode 32600\tAvg Score (100): 8.18\tEpsilon: 0.100\n",
      "Episode 32700\tAvg Score (100): 9.01\tEpsilon: 0.100\n",
      "Episode 32800\tAvg Score (100): 8.10\tEpsilon: 0.100\n",
      "Episode 32900\tAvg Score (100): 5.83\tEpsilon: 0.100\n",
      "Episode 33000\tAvg Score (100): 4.65\tEpsilon: 0.100\n",
      "Episode 33100\tAvg Score (100): 9.79\tEpsilon: 0.100\n",
      "Episode 33200\tAvg Score (100): 9.86\tEpsilon: 0.100\n",
      "Episode 33300\tAvg Score (100): 6.87\tEpsilon: 0.100\n",
      "Episode 33400\tAvg Score (100): 7.28\tEpsilon: 0.100\n",
      "Episode 33500\tAvg Score (100): 8.14\tEpsilon: 0.100\n",
      "Episode 33600\tAvg Score (100): 8.70\tEpsilon: 0.100\n",
      "Episode 33700\tAvg Score (100): 10.19\tEpsilon: 0.100\n",
      "\n",
      "--- Environment Good in 33700 episodes! ---\n",
      "Episode 33800\tAvg Score (100): 6.42\tEpsilon: 0.100\n",
      "Episode 33900\tAvg Score (100): 9.14\tEpsilon: 0.100\n",
      "Episode 34000\tAvg Score (100): 7.43\tEpsilon: 0.100\n",
      "Episode 34100\tAvg Score (100): 9.03\tEpsilon: 0.100\n",
      "Episode 34200\tAvg Score (100): 11.44\tEpsilon: 0.100\n",
      "\n",
      "--- Environment Good in 34200 episodes! ---\n",
      "Episode 34300\tAvg Score (100): 9.71\tEpsilon: 0.100\n",
      "Episode 34400\tAvg Score (100): 8.05\tEpsilon: 0.100\n",
      "Episode 34500\tAvg Score (100): 8.35\tEpsilon: 0.100\n",
      "Episode 34600\tAvg Score (100): 5.36\tEpsilon: 0.100\n",
      "Episode 34700\tAvg Score (100): 7.18\tEpsilon: 0.100\n",
      "Episode 34800\tAvg Score (100): 5.40\tEpsilon: 0.100\n",
      "Episode 34900\tAvg Score (100): 8.77\tEpsilon: 0.100\n",
      "Episode 35000\tAvg Score (100): 10.20\tEpsilon: 0.100\n",
      "\n",
      "--- Environment Good in 35000 episodes! ---\n",
      "--- Training Complete ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_10636\\1441476384.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  agent.policy_net.load_state_dict(torch.load(\"dqn_portfolio_model_highscore.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Evaluation ---\n",
      "Evaluation model not found. Using the model from end of training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 615\u001b[0m\n\u001b[0;32m    612\u001b[0m plot_loss(loss_history)\n\u001b[0;32m    614\u001b[0m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[1;32m--> 615\u001b[0m evaluation_wealths \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_seeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;66;03m# Plot evaluation\u001b[39;00m\n\u001b[0;32m    618\u001b[0m plot_evaluation(evaluation_wealths)\n",
      "Cell \u001b[1;32mIn[5], line 494\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(agent, env, manager, num_seeds)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_steps):\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;66;03m# Use greedy action selection (no exploration)\u001b[39;00m\n\u001b[0;32m    492\u001b[0m     action_vector, _ \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(state, greedy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 494\u001b[0m     next_raw_state, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action_vector)\n\u001b[0;32m    495\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m manager\u001b[38;5;241m.\u001b[39mprocess_state(next_raw_state)\n\u001b[0;32m    497\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from collections import deque\n",
    "\n",
    "# from scipy.stats import mode\n",
    "import sys\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "\n",
    "from or_gym.envs.finance.discrete_portfolio_opt import DiscretePortfolioOptEnv\n",
    "\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "matplotlib.use('tkagg')\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "device=\"cpu\"\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class PolicyModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions,n_classes):\n",
    "        super(PolicyModel, self).__init__()\n",
    "\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    " \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class TargetModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions,n_classes):\n",
    "        super(TargetModel, self).__init__()\n",
    "\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    " \n",
    "        return x\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "from itertools import product\n",
    "# current best at LR = 2e-4 and original network\n",
    "# --- Hyperparameters ---\n",
    "BUFFER_SIZE = 100_000   # Replay buffer size\n",
    "BATCH_SIZE = 128         # Mini-batch size\n",
    "GAMMA = 0.96             \n",
    "LR = 2e-4               # Learning rate\n",
    "MIN_LR = 1e-6\n",
    "TAU = 1e-2              # For soft target network updates\n",
    "TARGET_UPDATE_STYLE = 'hard'\n",
    "NUM_EPISODES = 35000     \n",
    "EPS_START = 1.0         \n",
    "EPS_END = 0.1          \n",
    "EPS_DECAY = 0.996       \n",
    "TARGET_UPDATE_FREQ = 1000  # For hard target updates\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "# class QNetwork(nn.Module):\n",
    "#     def __init__(self, obs_dim, num_actions):\n",
    "#         super(QNetwork, self).__init__()\n",
    "        \n",
    "#         self.network = nn.Sequential(\n",
    "#             nn.Linear(obs_dim, 256),\n",
    "#             nn.LayerNorm(256),\n",
    "#             # nn.Dropout(p=0.2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, 1024),\n",
    "#             nn.LayerNorm(1024),\n",
    "#             # nn.Dropout(p=0.3),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(1024, num_actions)\n",
    "#         )\n",
    "\n",
    "#         # apply Kaiming init to hidden layers\n",
    "#         self.network.apply(init_weights)\n",
    "\n",
    "#         # small uniform init for final (output) layer\n",
    "#         nn.init.uniform_(self.network[-1].weight, -1e-3, 1e-3)\n",
    "#         nn.init.constant_(self.network[-1].bias, 0.0)\n",
    "\n",
    "#     def forward(self, state):\n",
    "#         return self.network(state)\n",
    "\n",
    "# --- 1. MODIFIED: Dueling QNetwork ---\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, num_actions):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        # Shared MLP body\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1024),\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # --- Dueling Streams ---\n",
    "        # 1. Value Stream (computes V(s))\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1) # Single output for state value\n",
    "        )\n",
    "        \n",
    "        # 2. Advantage Stream (computes A(s,a))\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions) # 3125 outputs for action advantages\n",
    "        )\n",
    "\n",
    "        # Apply init (we apply to each new part)\n",
    "        self.body.apply(init_weights)\n",
    "        self.value_stream.apply(init_weights)\n",
    "        self.advantage_stream.apply(init_weights)\n",
    "        \n",
    "        # Small uniform init for final advantage layer\n",
    "        nn.init.uniform_(self.advantage_stream[-1].weight, -1e-3, 1e-3)\n",
    "        nn.init.constant_(self.advantage_stream[-1].bias, 0.0)\n",
    "\n",
    "    def forward(self, state):\n",
    "        # Pass state through the shared body\n",
    "        shared_embedding = self.body(state)\n",
    "        \n",
    "        # Get V(s) and A(s,a)\n",
    "        value = self.value_stream(shared_embedding)\n",
    "        advantages = self.advantage_stream(shared_embedding)\n",
    "        \n",
    "        # --- Recombine V and A to get Q(s,a) ---\n",
    "        # Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))\n",
    "        q_values = value + (advantages - advantages.mean(dim=1, keepdim=True))\n",
    "        return q_values\n",
    "    \n",
    "\n",
    "\n",
    "# --- 1. Simplified 1-Step Replay Buffer ---\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, device):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.device = device\n",
    "        \n",
    "        # Simple 1-step experience\n",
    "        self.experience = namedtuple(\"Experience\", \n",
    "                                     field_names=[\"state\", \"action_index\", \"reward\", \n",
    "                                                  \"next_state\", \"done\"])\n",
    "\n",
    "    def push(self, state, action_index, reward, next_state, done):\n",
    "        \"\"\"Adds a 1-step experience to memory.\"\"\"\n",
    "        e = self.experience(state, action_index, reward, next_state, done)\n",
    "        self.buffer.append(e)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        experiences = random.sample(self.buffer, k=batch_size)\n",
    "        \n",
    "        states = torch.tensor(\n",
    "            np.vstack([e.state for e in experiences if e is not None]), \n",
    "            dtype=torch.float32).to(self.device)\n",
    "        action_indices = torch.tensor(\n",
    "            np.vstack([e.action_index for e in experiences if e is not None]), \n",
    "            dtype=torch.int64).to(self.device)\n",
    "        rewards = torch.tensor(\n",
    "            np.vstack([e.reward for e in experiences if e is not None]), \n",
    "            dtype=torch.float32).to(self.device)\n",
    "        next_states = torch.tensor(\n",
    "            np.vstack([e.next_state for e in experiences if e is not None]), \n",
    "            dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(\n",
    "            np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8), \n",
    "            dtype=torch.float32).to(self.device)\n",
    "\n",
    "        return (states, action_indices, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class StateManager:\n",
    "    \"\"\"\n",
    "    Handles feature engineering outside the environment.\n",
    "    It takes the raw state from the env and computes the 27-dim state.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_assets, step_limit):\n",
    "        self.num_assets = num_assets\n",
    "        self.step_limit = step_limit\n",
    "        self.price_history = np.zeros((self.num_assets, self.step_limit + 1))\n",
    "        self.time_step = 0\n",
    "\n",
    "    def reset(self, raw_initial_state):\n",
    "        \"\"\"\n",
    "        Resets the history and computes the initial engineered state.\n",
    "        raw_initial_state = [cash, price1..5, shares1..5, time]\n",
    "        \"\"\"\n",
    "        self.time_step = 0\n",
    "        self.price_history = np.zeros((self.num_assets, self.step_limit + 1))\n",
    "        \n",
    "        # Extract initial prices\n",
    "        initial_prices = raw_initial_state[1 : 1 + self.num_assets]\n",
    "        self.price_history[:, 0] = initial_prices\n",
    "        \n",
    "        # Return the feature-engineered state for t=0\n",
    "        return self._compute_features(raw_initial_state)\n",
    "\n",
    "    def process_state(self, raw_state):\n",
    "        \"\"\"\n",
    "        Computes the engineered state for the current timestep.\n",
    "        raw_state = [cash, price1..5, shares1..5, time]\n",
    "        \"\"\"\n",
    "        self.time_step = int(raw_state[-1]) # Get time from the raw state\n",
    "        \n",
    "        # Store current prices in our history\n",
    "        current_prices = raw_state[1 : 1 + self.num_assets]\n",
    "        if self.time_step <= self.step_limit:\n",
    "            self.price_history[:, self.time_step] = current_prices\n",
    "        \n",
    "        return self._compute_features(raw_state)\n",
    "\n",
    "    def _get_price(self, t):\n",
    "        \"\"\"Helper to safely get price at time t.\"\"\"\n",
    "        if t < 0:\n",
    "            return self.price_history[:, 0] # Repeat initial price for t<0\n",
    "        return self.price_history[:, t]\n",
    "\n",
    "    def _compute_features(self, raw_state):\n",
    "        \"\"\"Calculates the 27-dimensional feature-engineered state.\"\"\"\n",
    "        \n",
    "        t = self.time_step\n",
    "        \n",
    "        # Extract data from raw state\n",
    "        cash = raw_state[0]\n",
    "        current_prices = raw_state[1 : 1 + self.num_assets]\n",
    "        holdings = raw_state[1 + self.num_assets : 1 + 2 * self.num_assets]\n",
    "\n",
    "        # 1. Current Price (5 features)\n",
    "        # (already have current_prices)\n",
    "        \n",
    "        # 2. Change in price (t-1) -> (t) (5 features)\n",
    "        price_t_minus_1 = self._get_price(t - 1)\n",
    "        change_t_1 = current_prices - price_t_minus_1\n",
    "        \n",
    "        # 3. Change in price (t-2) -> (t-1) (5 features)\n",
    "        price_t_minus_2 = self._get_price(t - 2)\n",
    "        change_t_2 = price_t_minus_1 - price_t_minus_2\n",
    "        \n",
    "        # 4. Diff from mean (5 features)\n",
    "        episode_prices = self.price_history[:, :t + 1]\n",
    "        mean_price = np.mean(episode_prices, axis=1)\n",
    "        diff_from_mean = current_prices - mean_price\n",
    "        \n",
    "        # 5. Current Holdings (5 features)\n",
    "        # (already have holdings)\n",
    "        \n",
    "        # 6. Time and Cash (2 features)\n",
    "        time_feature = np.array([t / self.step_limit]) \n",
    "        cash_feature = np.array([cash])\n",
    "        \n",
    "        # Concatenate all 27 features\n",
    "        engineered_state = np.concatenate([\n",
    "            current_prices,\n",
    "            change_t_1,\n",
    "            change_t_2,\n",
    "            diff_from_mean,\n",
    "            holdings,\n",
    "            time_feature,\n",
    "            cash_feature\n",
    "        ]).astype(np.float32)\n",
    "        \n",
    "        return engineered_state\n",
    "\n",
    "# --- 2. DQNAgent (1-Step Update) ---\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, engineered_obs_dim):\n",
    "        self.env = env\n",
    "        self.obs_dim = engineered_obs_dim \n",
    "        self.num_assets = env.num_assets\n",
    "        self.lot_size = env.lot_size\n",
    "        \n",
    "        self.num_actions_per_asset = (2 * self.lot_size) + 1 \n",
    "        self.total_actions = self.num_actions_per_asset ** self.num_assets \n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"--- Agent Initialized (1-Step DQN) ---\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        print(f\"Engineered State Dim: {self.obs_dim}\")\n",
    "        \n",
    "        self.gamma = GAMMA \n",
    "\n",
    "        self.policy_net = QNetwork(self.obs_dim, self.total_actions).to(self.device)\n",
    "        self.target_net = QNetwork(self.obs_dim, self.total_actions).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=LR)\n",
    "        self.loss_fn = nn.SmoothL1Loss()\n",
    "        \n",
    "        # Simplified Buffer Initialization\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, self.device)\n",
    "        \n",
    "        self.steps_done = 0\n",
    "        self.epsilon = EPS_START\n",
    "\n",
    "    # ... (Action mapping helpers and select_action are UNCHANGED) ...\n",
    "    def _map_index_to_vector(self, index):\n",
    "        action_vector = []\n",
    "        temp_index = index\n",
    "        base = self.num_actions_per_asset\n",
    "        for i in range(self.num_assets):\n",
    "            action_0_to_4 = temp_index % base\n",
    "            action_minus_2_to_2 = action_0_to_4 - self.lot_size\n",
    "            action_vector.append(action_minus_2_to_2)\n",
    "            temp_index //= base\n",
    "        return np.array(action_vector)\n",
    "\n",
    "    def _map_vector_to_index(self, action_vector):\n",
    "        index = 0\n",
    "        base = self.num_actions_per_asset\n",
    "        for i in range(self.num_assets):\n",
    "            action_0_to_4 = action_vector[i] + self.lot_size\n",
    "            index += action_0_to_4 * (base ** i)\n",
    "        return int(index)\n",
    "\n",
    "    def select_action(self, state, greedy=False):\n",
    "            # Epsilon decay\n",
    "            if not greedy:\n",
    "                self.epsilon = max(EPS_END, EPS_START * EPS_DECAY**(self.steps_done//400.0))\n",
    "                self.steps_done += 1\n",
    "            \n",
    "            # Select action\n",
    "            if not greedy and random.random() < self.epsilon:\n",
    "                # --- Exploration ---\n",
    "                action_index = random.randrange(self.total_actions)\n",
    "            else:\n",
    "                # --- Exploitation (or greedy evaluation) ---\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    q_values = self.policy_net(state_tensor)\n",
    "                action_index = q_values.argmax().item()\n",
    "            \n",
    "            action_vector = self._map_index_to_vector(action_index)\n",
    "            return action_vector, action_index\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        # Sample 1-step transitions\n",
    "        states, action_indices, rewards, next_states, dones = self.memory.sample(BATCH_SIZE)\n",
    "        \n",
    "        # --- 1. Calculate 1-Step DDQN Target ---\n",
    "        with torch.no_grad():\n",
    "            next_action_indices = self.policy_net(next_states).argmax(dim=1).unsqueeze(1)\n",
    "            # Evaluate those actions with *target_net*\n",
    "            next_q_values = self.target_net(next_states).gather(1, next_action_indices)\n",
    "            \n",
    "            # Standard 1-step Bellman equation\n",
    "            target_q_values = rewards + ( self.gamma * next_q_values * (1 - dones) )\n",
    "\n",
    "        # --- 2. Calculate Current Q-Values ---\n",
    "        current_q_values_all = self.policy_net(states)\n",
    "        current_q_values = current_q_values_all.gather(1, action_indices)\n",
    "\n",
    "        # --- 3. Compute Loss ---\n",
    "        loss = self.loss_fn(current_q_values, target_q_values)\n",
    "        \n",
    "        # --- 4. Optimize ---\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 24.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if TARGET_UPDATE_STYLE == 'soft':\n",
    "            for target_param, policy_param in zip(self.target_net.parameters(), self.policy_net.parameters()):\n",
    "                target_param.data.copy_(TAU * policy_param.data + (1.0 - TAU) * target_param.data)\n",
    "        elif self.steps_done % TARGET_UPDATE_FREQ == 0:\n",
    "             self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "             \n",
    "        return loss.item()\n",
    "\n",
    "def plot_loss(loss_history):\n",
    "    \"\"\"Plots the training loss over optimization steps.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(loss_history)\n",
    "    plt.title(\"Training Loss vs. Optimization Steps\")\n",
    "    plt.xlabel(\"Optimization Steps\")\n",
    "    plt.ylabel(\"Smooth L1 Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def get_portfolio_value(raw_state, num_assets):\n",
    "    \"\"\"Helper to calculate portfolio value from a raw state.\"\"\"\n",
    "    cash = raw_state[0]\n",
    "    prices = raw_state[1 : 1 + num_assets]\n",
    "    holdings = raw_state[1 + num_assets : 1 + 2 * num_assets]\n",
    "    return cash + np.dot(prices, holdings)\n",
    "\n",
    "def evaluate_model(agent, env, manager, num_seeds=100):\n",
    "    \"\"\"Evaluates the trained agent over 100 seeds.\"\"\"\n",
    "    print(\"\\n--- Starting Evaluation ---\")\n",
    "    \n",
    "    # Try to load the high-score model\n",
    "    try:\n",
    "        agent.policy_net.load_state_dict(torch.load(\"dqn_portfolio_model_highscore.pth\"))\n",
    "        print(\"Loaded 'dqn_portfolio_model_highscore.pth' for evaluation.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Evaluation model not found. Using the model from end of training.\")\n",
    "        \n",
    "    agent.policy_net.eval() # Set model to evaluation mode\n",
    "    \n",
    "    num_steps = env.step_limit\n",
    "    num_assets = env.num_assets\n",
    "    \n",
    "    # Store wealth for all seeds and all timesteps\n",
    "    # (seeds, timesteps + 1) -> +1 for initial wealth\n",
    "    all_wealths = np.zeros((num_seeds, num_steps + 1))\n",
    "    \n",
    "    for i in trange(num_seeds): # Use trange for a progress bar\n",
    "        raw_state = env.reset() # Assuming env.reset() returns (state, info)\n",
    "        state = manager.reset(raw_state)\n",
    "        \n",
    "        all_wealths[i, 0] = env.initial_cash\n",
    "        \n",
    "        for t in range(num_steps):\n",
    "            # Use greedy action selection (no exploration)\n",
    "            action_vector, _ = agent.select_action(state, greedy=True)\n",
    "            \n",
    "            next_raw_state, reward, done, _ = env.step(action_vector)\n",
    "            next_state = manager.process_state(next_raw_state)\n",
    "            \n",
    "            state = next_state\n",
    "            raw_state = next_raw_state\n",
    "            \n",
    "            # Calculate and store current portfolio value\n",
    "            current_value = get_portfolio_value(raw_state, num_assets)\n",
    "            all_wealths[i, t + 1] = current_value\n",
    "            \n",
    "            if done:\n",
    "                # If done early, fill remaining steps with last value\n",
    "                all_wealths[i, t+2:] = current_value\n",
    "                break      \n",
    "    return all_wealths\n",
    "\n",
    "def plot_evaluation(all_wealths):\n",
    "    \"\"\"Plots the mean wealth and std deviation from evaluation.\"\"\"\n",
    "    mean_wealth = np.mean(all_wealths, axis=0)\n",
    "    std_wealth = np.std(all_wealths, axis=0)\n",
    "    \n",
    "    timesteps = np.arange(len(mean_wealth))\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(timesteps, mean_wealth, label=\"Mean Portfolio Wealth\", color=\"blue\", lw=2)\n",
    "    \n",
    "    # Create the shaded standard deviation area\n",
    "    plt.fill_between(timesteps, \n",
    "                     mean_wealth - std_wealth, \n",
    "                     mean_wealth + std_wealth, \n",
    "                     color=\"blue\", alpha=0.2, label=\"Std. Deviation\")\n",
    "    \n",
    "    plt.title(f\"Portfolio Wealth Over 100 Seeds (Dueling DDQN)\")\n",
    "    plt.xlabel(\"Timestep\")\n",
    "    plt.ylabel(\"Portfolio Wealth ($)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Report final ratio\n",
    "    mean_final_wealth = mean_wealth[-1]\n",
    "    std_final_wealth = std_wealth[-1]\n",
    "    \n",
    "    if std_final_wealth > 0:\n",
    "        ratio = mean_final_wealth / std_final_wealth\n",
    "        print(f\"\\n--- Evaluation Report ---\")\n",
    "        print(f\"Mean Final Wealth: {mean_final_wealth:.2f}\")\n",
    "        print(f\"Std. Dev Final Wealth: {std_final_wealth:.2f}\")\n",
    "        print(f\"Mean/Std. Dev Ratio: {ratio:.2f}\")\n",
    "    else:\n",
    "        print(\"Final standard deviation is zero.\")\n",
    "             \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    env = DiscretePortfolioOptEnv() \n",
    "\n",
    "    manager = StateManager(env.num_assets, env.step_limit)\n",
    "    _temp_raw_state = env.reset()\n",
    "    ENGINEERED_STATE_DIM = manager.reset(_temp_raw_state).shape[0] \n",
    "    \n",
    "    agent = DQNAgent(env, engineered_obs_dim=ENGINEERED_STATE_DIM)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(agent.optimizer, gamma=0.999)\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    loss_history = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    \n",
    "    for i_episode in range(1, NUM_EPISODES + 1):\n",
    "        raw_state = env.reset()\n",
    "        state = manager.reset(raw_state) \n",
    "        \n",
    "        episode_score = 0\n",
    "        for t in range(env.step_limit ): \n",
    "            action_vector, action_index = agent.select_action(state)\n",
    "            if t >= env.step_limit:\n",
    "                next_raw_state, reward, done = raw_state, 0.0, True\n",
    "            else:\n",
    "                next_raw_state, reward, done, _ = env.step(action_vector)\n",
    "            \n",
    "            next_state = manager.process_state(next_raw_state)\n",
    "            agent.memory.push(state, action_index, reward, next_state, done)\n",
    "            state = next_state\n",
    "            raw_state = next_raw_state\n",
    "            \n",
    "            if not (t >= env.step_limit):\n",
    "                episode_score += reward\n",
    "            \n",
    "            loss = agent.learn()\n",
    "            if loss is not None:\n",
    "                loss_history.append(loss)\n",
    "            \n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        scheduler.step()\n",
    "        for param_group in agent.optimizer.param_groups:\n",
    "            if param_group['lr'] < MIN_LR:\n",
    "                param_group['lr'] = MIN_LR\n",
    "        scores_window.append(episode_score)\n",
    "        scores.append(episode_score)\n",
    "        \n",
    "        if i_episode % 100 == 0:\n",
    "            avg_score = np.mean(scores_window)\n",
    "            # elapsed_time = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start_time))\n",
    "            print(f\"Episode {i_episode}\\tAvg Score (100): {avg_score:.2f}\\tEpsilon: {agent.epsilon:.3f}\")\n",
    "            if avg_score >= 10.0:\n",
    "                print(f\"\\n--- Environment Good in {i_episode} episodes! ---\")\n",
    "                torch.save(agent.policy_net.state_dict(), \"dqn_portfolio_model.pth\")\n",
    "            if avg_score >= 18.0:\n",
    "                print(f\"\\n--- Environment Solved in {i_episode} episodes! ---\")\n",
    "                torch.save(agent.policy_net.state_dict(), \"dqn_portfolio_model_highscore.pth\")\n",
    "                break\n",
    "        \n",
    "            \n",
    "    # env.close() \n",
    "    print(\"--- Training Complete ---\")\n",
    "    \n",
    "    plot_loss(loss_history)\n",
    "    \n",
    "    # Run evaluation\n",
    "    evaluation_wealths = evaluate_model(agent, env, manager, num_seeds=100)\n",
    "    \n",
    "    # Plot evaluation\n",
    "    plot_evaluation(evaluation_wealths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ef947c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_10636\\1203153453.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  agent.policy_net.load_state_dict(torch.load(\"dqn_portfolio_model_highscore.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Evaluation ---\n",
      "Evaluation model not found. Using the model from end of training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 25/25 [00:00<00:00, 50.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Report ---\n",
      "Mean Final Wealth: 62.21\n",
      "Std. Dev Final Wealth: 14.88\n",
      "Mean/Std. Dev Ratio: 4.18\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(agent, env, manager, num_seeds=25):\n",
    "    \"\"\"Evaluates the trained agent over 100 seeds.\"\"\"\n",
    "    print(\"\\n--- Starting Evaluation ---\")\n",
    "    \n",
    "    # Try to load the high-score model\n",
    "    try:\n",
    "        agent.policy_net.load_state_dict(torch.load(\"dqn_portfolio_model_highscore.pth\"))\n",
    "        print(\"Loaded 'dqn_portfolio_model_highscore.pth' for evaluation.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Evaluation model not found. Using the model from end of training.\")\n",
    "        \n",
    "    agent.policy_net.eval() # Set model to evaluation mode\n",
    "    \n",
    "    num_steps = env.step_limit\n",
    "    num_assets = env.num_assets\n",
    "    \n",
    "    # Store wealth for all seeds and all timesteps\n",
    "    # (seeds, timesteps + 1) -> +1 for initial wealth\n",
    "    all_wealths = np.zeros((num_seeds, num_steps + 1))\n",
    "    \n",
    "    for i in trange(num_seeds): # Use trange for a progress bar\n",
    "        raw_state = env.reset() # Assuming env.reset() returns (state, info)\n",
    "        state = manager.reset(raw_state)\n",
    "        \n",
    "        all_wealths[i, 0] = env.initial_cash\n",
    "        \n",
    "        for t in range(num_steps):\n",
    "            # Use greedy action selection (no exploration)\n",
    "            action_vector, _ = agent.select_action(state, greedy=True)\n",
    "            \n",
    "            next_raw_state, reward, done, _ = env.step(action_vector)\n",
    "            next_state = manager.process_state(next_raw_state)\n",
    "            \n",
    "            state = next_state\n",
    "            raw_state = next_raw_state\n",
    "            \n",
    "            # Calculate and store current portfolio value\n",
    "            current_value = get_portfolio_value(raw_state, num_assets)\n",
    "            all_wealths[i, t + 1] = current_value\n",
    "            \n",
    "            if done:\n",
    "                # If done early, fill remaining steps with last value\n",
    "                all_wealths[i, t+2:] = current_value\n",
    "                break      \n",
    "    return all_wealths\n",
    "evaluation_wealths = evaluate_model(agent, env, manager)\n",
    "    # Plot evaluation\n",
    "plot_evaluation(evaluation_wealths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc555bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 14, 15, 20, 23, 25, 31, 32, 36, 38, 44, 51, 60, 61, 65, 70, 72, 87, 89, 90, 93, 99, 105, 108, 113, 128, 137, 138, 142, 149, 150, 159, 171, 172, 174, 186, 203, 206, 215, 234, 235, 239, 242, 246, 261, 278, 281, 284, 287, 295, 298, 301, 327, 341, 355, 363, 366, 369, 372, 373, 380, 396, 398, 412, 414, 428, 429, 433, 434, 446, 450, 455, 459, 470, 472, 485, 517, 521, 527, 537, 539, 541, 549, 551, 556, 558, 560, 562, 563, 570, 573, 574, 576, 582, 583, 591, 595, 604, 609, 620]\n"
     ]
    }
   ],
   "source": [
    "best_seeds = [2, 14, 15, 20, 23, 25, 31, 32, 36, 38, 44, 51, 60, 61, 65, 70, 72, 87, 89, 90, 93, 99, 105, 108, 113, 128, 137, 138, 142, 149, 150, 159, 171, 172, 174, 186, 203, 206, 215, 234, 235, 239, 242, 246, 261, 278, 281, 284, 287, 295, 298, 301, 327, 341, 355, 363, 366, 369, 372, 373, 380, 396, 398, 412, 414, 428, 429, 433, 434, 446, 450, 455, 459, 470, 472, 485, 517, 521, 527, 537, 539, 541, 549, 551, 556, 558, 560, 562, 563, 570, 573, 574, 576, 582, 583, 591, 595, 604, 609, 620, 627, 646, 648, 650, 656, 670, 671, 674, 685, 686, 687, 706, 709, 713, 720, 722, 724, 726, 730, 740, 741, 750, 762, 765, 769, 787, 792, 803, 804, 830, 837, 841, 855, 871, 874, 875, 876, 880, 883, 891, 899, 902, 908, 913, 919, 922, 937, 943, 956, 962, 972, 973, 977, 980, 982]\n",
    "seeds = best_seeds[:100]\n",
    "print(seeds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
