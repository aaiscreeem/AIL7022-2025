{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe518a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Agent Initialized (1-Step DQN) ---\n",
      "Device: cuda\n",
      "Engineered State Dim: 27\n",
      "\n",
      "--- Starting Training ---\n",
      "Episode 100\tAvg Score (100): -9.71\tEpsilon: 0.994\n",
      "Episode 200\tAvg Score (100): -9.89\tEpsilon: 0.988\n",
      "Episode 300\tAvg Score (100): -9.87\tEpsilon: 0.979\n",
      "Episode 400\tAvg Score (100): -9.73\tEpsilon: 0.973\n",
      "Episode 500\tAvg Score (100): -10.01\tEpsilon: 0.965\n",
      "Episode 600\tAvg Score (100): -9.57\tEpsilon: 0.959\n",
      "Episode 700\tAvg Score (100): -9.55\tEpsilon: 0.950\n",
      "Episode 800\tAvg Score (100): -9.56\tEpsilon: 0.945\n",
      "Episode 900\tAvg Score (100): -9.46\tEpsilon: 0.936\n",
      "Episode 1000\tAvg Score (100): -9.21\tEpsilon: 0.930\n",
      "Episode 1100\tAvg Score (100): -9.48\tEpsilon: 0.922\n",
      "Episode 1200\tAvg Score (100): -9.31\tEpsilon: 0.917\n",
      "Episode 1300\tAvg Score (100): -9.36\tEpsilon: 0.908\n",
      "Episode 1400\tAvg Score (100): -9.08\tEpsilon: 0.903\n",
      "Episode 1500\tAvg Score (100): -8.95\tEpsilon: 0.895\n",
      "Episode 1600\tAvg Score (100): -8.72\tEpsilon: 0.889\n",
      "Episode 1700\tAvg Score (100): -8.63\tEpsilon: 0.881\n",
      "Episode 1800\tAvg Score (100): -8.89\tEpsilon: 0.876\n",
      "Episode 1900\tAvg Score (100): -8.94\tEpsilon: 0.868\n",
      "Episode 2000\tAvg Score (100): -8.76\tEpsilon: 0.863\n",
      "Episode 2100\tAvg Score (100): -8.58\tEpsilon: 0.855\n",
      "Episode 2200\tAvg Score (100): -8.59\tEpsilon: 0.850\n",
      "Episode 2300\tAvg Score (100): -8.49\tEpsilon: 0.843\n",
      "Episode 2400\tAvg Score (100): -8.37\tEpsilon: 0.838\n",
      "Episode 2500\tAvg Score (100): -8.86\tEpsilon: 0.830\n",
      "Episode 2600\tAvg Score (100): -8.53\tEpsilon: 0.825\n",
      "Episode 2700\tAvg Score (100): -8.51\tEpsilon: 0.818\n",
      "Episode 2800\tAvg Score (100): -8.34\tEpsilon: 0.813\n",
      "Episode 2900\tAvg Score (100): -8.53\tEpsilon: 0.805\n",
      "Episode 3000\tAvg Score (100): -8.34\tEpsilon: 0.801\n",
      "Episode 3100\tAvg Score (100): -8.70\tEpsilon: 0.793\n",
      "Episode 3200\tAvg Score (100): -8.36\tEpsilon: 0.789\n",
      "Episode 3300\tAvg Score (100): -8.18\tEpsilon: 0.782\n",
      "Episode 3400\tAvg Score (100): -8.07\tEpsilon: 0.777\n",
      "Episode 3500\tAvg Score (100): -8.10\tEpsilon: 0.770\n",
      "Episode 3600\tAvg Score (100): -7.87\tEpsilon: 0.765\n",
      "Episode 3700\tAvg Score (100): -7.82\tEpsilon: 0.758\n",
      "Episode 3800\tAvg Score (100): -7.93\tEpsilon: 0.754\n",
      "Episode 3900\tAvg Score (100): -7.31\tEpsilon: 0.747\n",
      "Episode 4000\tAvg Score (100): -7.32\tEpsilon: 0.743\n",
      "Episode 4100\tAvg Score (100): -7.83\tEpsilon: 0.736\n",
      "Episode 4200\tAvg Score (100): -7.15\tEpsilon: 0.732\n",
      "Episode 4300\tAvg Score (100): -7.38\tEpsilon: 0.725\n",
      "Episode 4400\tAvg Score (100): -7.29\tEpsilon: 0.721\n",
      "Episode 4500\tAvg Score (100): -7.43\tEpsilon: 0.714\n",
      "Episode 4600\tAvg Score (100): -7.22\tEpsilon: 0.710\n",
      "Episode 4700\tAvg Score (100): -7.24\tEpsilon: 0.704\n",
      "Episode 4800\tAvg Score (100): -6.90\tEpsilon: 0.699\n",
      "Episode 4900\tAvg Score (100): -7.03\tEpsilon: 0.693\n",
      "Episode 5000\tAvg Score (100): -6.96\tEpsilon: 0.689\n",
      "Episode 5100\tAvg Score (100): -6.74\tEpsilon: 0.683\n",
      "Episode 5200\tAvg Score (100): -6.71\tEpsilon: 0.679\n",
      "Episode 5300\tAvg Score (100): -7.18\tEpsilon: 0.673\n",
      "Episode 5400\tAvg Score (100): -6.85\tEpsilon: 0.669\n",
      "Episode 5500\tAvg Score (100): -6.92\tEpsilon: 0.663\n",
      "Episode 5600\tAvg Score (100): -6.76\tEpsilon: 0.659\n",
      "Episode 5700\tAvg Score (100): -6.72\tEpsilon: 0.653\n",
      "Episode 5800\tAvg Score (100): -6.37\tEpsilon: 0.649\n",
      "Episode 5900\tAvg Score (100): -6.47\tEpsilon: 0.643\n",
      "Episode 6000\tAvg Score (100): -6.54\tEpsilon: 0.639\n",
      "Episode 6100\tAvg Score (100): -6.25\tEpsilon: 0.633\n",
      "Episode 6200\tAvg Score (100): -6.46\tEpsilon: 0.630\n",
      "Episode 6300\tAvg Score (100): -6.58\tEpsilon: 0.624\n",
      "Episode 6400\tAvg Score (100): -6.48\tEpsilon: 0.620\n",
      "Episode 6500\tAvg Score (100): -6.51\tEpsilon: 0.615\n",
      "Episode 6600\tAvg Score (100): -5.75\tEpsilon: 0.611\n",
      "Episode 6700\tAvg Score (100): -5.94\tEpsilon: 0.605\n",
      "Episode 6800\tAvg Score (100): -6.05\tEpsilon: 0.602\n",
      "Episode 6900\tAvg Score (100): -6.05\tEpsilon: 0.596\n",
      "Episode 7000\tAvg Score (100): -5.83\tEpsilon: 0.593\n",
      "Episode 7100\tAvg Score (100): -5.82\tEpsilon: 0.588\n",
      "Episode 7200\tAvg Score (100): -5.88\tEpsilon: 0.584\n",
      "Episode 7300\tAvg Score (100): -5.55\tEpsilon: 0.579\n",
      "Episode 7400\tAvg Score (100): -5.37\tEpsilon: 0.575\n",
      "Episode 7500\tAvg Score (100): -5.60\tEpsilon: 0.570\n",
      "Episode 7600\tAvg Score (100): -5.54\tEpsilon: 0.567\n",
      "Episode 7700\tAvg Score (100): -5.95\tEpsilon: 0.562\n",
      "Episode 7800\tAvg Score (100): -5.61\tEpsilon: 0.558\n",
      "Episode 7900\tAvg Score (100): -5.57\tEpsilon: 0.553\n",
      "Episode 8000\tAvg Score (100): -5.76\tEpsilon: 0.550\n",
      "Episode 8100\tAvg Score (100): -5.81\tEpsilon: 0.545\n",
      "Episode 8200\tAvg Score (100): -5.80\tEpsilon: 0.542\n",
      "Episode 8300\tAvg Score (100): -5.12\tEpsilon: 0.537\n",
      "Episode 8400\tAvg Score (100): -5.34\tEpsilon: 0.534\n",
      "Episode 8500\tAvg Score (100): -5.14\tEpsilon: 0.529\n",
      "Episode 8600\tAvg Score (100): -5.25\tEpsilon: 0.526\n",
      "Episode 8700\tAvg Score (100): -5.38\tEpsilon: 0.521\n",
      "Episode 8800\tAvg Score (100): -5.30\tEpsilon: 0.518\n",
      "Episode 8900\tAvg Score (100): -5.30\tEpsilon: 0.513\n",
      "Episode 9000\tAvg Score (100): -5.01\tEpsilon: 0.510\n",
      "Episode 9100\tAvg Score (100): -4.84\tEpsilon: 0.506\n",
      "Episode 9200\tAvg Score (100): -5.20\tEpsilon: 0.503\n",
      "Episode 9300\tAvg Score (100): -4.78\tEpsilon: 0.498\n",
      "Episode 9400\tAvg Score (100): -4.82\tEpsilon: 0.495\n",
      "Episode 9500\tAvg Score (100): -4.64\tEpsilon: 0.491\n",
      "Episode 9600\tAvg Score (100): -4.38\tEpsilon: 0.488\n",
      "Episode 9700\tAvg Score (100): -4.80\tEpsilon: 0.483\n",
      "Episode 9800\tAvg Score (100): -4.80\tEpsilon: 0.480\n",
      "Episode 9900\tAvg Score (100): -4.48\tEpsilon: 0.476\n",
      "Episode 10000\tAvg Score (100): -4.57\tEpsilon: 0.473\n",
      "Episode 10100\tAvg Score (100): -4.83\tEpsilon: 0.469\n",
      "Episode 10200\tAvg Score (100): -4.62\tEpsilon: 0.466\n",
      "Episode 10300\tAvg Score (100): -4.37\tEpsilon: 0.462\n",
      "Episode 10400\tAvg Score (100): -4.25\tEpsilon: 0.459\n",
      "Episode 10500\tAvg Score (100): -4.49\tEpsilon: 0.455\n",
      "Episode 10600\tAvg Score (100): -4.66\tEpsilon: 0.452\n",
      "Episode 10700\tAvg Score (100): -4.25\tEpsilon: 0.448\n",
      "Episode 10800\tAvg Score (100): -4.62\tEpsilon: 0.446\n",
      "Episode 10900\tAvg Score (100): -4.32\tEpsilon: 0.442\n",
      "Episode 11000\tAvg Score (100): -4.22\tEpsilon: 0.439\n",
      "Episode 11100\tAvg Score (100): -4.48\tEpsilon: 0.435\n",
      "Episode 11200\tAvg Score (100): -4.36\tEpsilon: 0.432\n",
      "Episode 11300\tAvg Score (100): -4.18\tEpsilon: 0.429\n",
      "Episode 11400\tAvg Score (100): -3.85\tEpsilon: 0.426\n",
      "Episode 11500\tAvg Score (100): -4.62\tEpsilon: 0.422\n",
      "Episode 11600\tAvg Score (100): -4.01\tEpsilon: 0.420\n",
      "Episode 11700\tAvg Score (100): -4.14\tEpsilon: 0.416\n",
      "Episode 11800\tAvg Score (100): -4.07\tEpsilon: 0.413\n",
      "Episode 11900\tAvg Score (100): -3.79\tEpsilon: 0.410\n",
      "Episode 12000\tAvg Score (100): -4.08\tEpsilon: 0.407\n",
      "Episode 12100\tAvg Score (100): -4.02\tEpsilon: 0.404\n",
      "Episode 12200\tAvg Score (100): -4.07\tEpsilon: 0.401\n",
      "Episode 12300\tAvg Score (100): -3.98\tEpsilon: 0.398\n",
      "Episode 12400\tAvg Score (100): -3.82\tEpsilon: 0.395\n",
      "Episode 12500\tAvg Score (100): -4.11\tEpsilon: 0.392\n",
      "Episode 12600\tAvg Score (100): -3.66\tEpsilon: 0.389\n",
      "Episode 12700\tAvg Score (100): -3.83\tEpsilon: 0.386\n",
      "Episode 12800\tAvg Score (100): -3.18\tEpsilon: 0.383\n",
      "Episode 12900\tAvg Score (100): -4.05\tEpsilon: 0.380\n",
      "Episode 13000\tAvg Score (100): -3.72\tEpsilon: 0.378\n",
      "Episode 13100\tAvg Score (100): -3.83\tEpsilon: 0.374\n",
      "Episode 13200\tAvg Score (100): -3.73\tEpsilon: 0.372\n",
      "Episode 13300\tAvg Score (100): -3.52\tEpsilon: 0.369\n",
      "Episode 13400\tAvg Score (100): -3.59\tEpsilon: 0.367\n",
      "Episode 13500\tAvg Score (100): -3.72\tEpsilon: 0.363\n",
      "Episode 13600\tAvg Score (100): -3.48\tEpsilon: 0.361\n",
      "Episode 13700\tAvg Score (100): -3.46\tEpsilon: 0.358\n",
      "Episode 13800\tAvg Score (100): -3.38\tEpsilon: 0.356\n",
      "Episode 13900\tAvg Score (100): -3.20\tEpsilon: 0.353\n",
      "Episode 14000\tAvg Score (100): -3.44\tEpsilon: 0.350\n",
      "Episode 14100\tAvg Score (100): -3.47\tEpsilon: 0.347\n",
      "Episode 14200\tAvg Score (100): -3.25\tEpsilon: 0.345\n",
      "Episode 14300\tAvg Score (100): -3.40\tEpsilon: 0.342\n",
      "Episode 14400\tAvg Score (100): -3.23\tEpsilon: 0.340\n",
      "Episode 14500\tAvg Score (100): -3.07\tEpsilon: 0.337\n",
      "Episode 14600\tAvg Score (100): -3.43\tEpsilon: 0.335\n",
      "Episode 14700\tAvg Score (100): -3.30\tEpsilon: 0.332\n",
      "Episode 14800\tAvg Score (100): -2.75\tEpsilon: 0.330\n",
      "Episode 14900\tAvg Score (100): -2.80\tEpsilon: 0.327\n",
      "Episode 15000\tAvg Score (100): -2.84\tEpsilon: 0.325\n",
      "Episode 15100\tAvg Score (100): -2.94\tEpsilon: 0.322\n",
      "Episode 15200\tAvg Score (100): -3.31\tEpsilon: 0.320\n",
      "Episode 15300\tAvg Score (100): -3.19\tEpsilon: 0.317\n",
      "Episode 15400\tAvg Score (100): -2.84\tEpsilon: 0.315\n",
      "Episode 15500\tAvg Score (100): -2.88\tEpsilon: 0.313\n",
      "Episode 15600\tAvg Score (100): -3.06\tEpsilon: 0.311\n",
      "Episode 15700\tAvg Score (100): -2.81\tEpsilon: 0.308\n",
      "Episode 15800\tAvg Score (100): -2.83\tEpsilon: 0.306\n",
      "Episode 15900\tAvg Score (100): -2.56\tEpsilon: 0.303\n",
      "Episode 16000\tAvg Score (100): -3.23\tEpsilon: 0.302\n",
      "Episode 16100\tAvg Score (100): -2.79\tEpsilon: 0.299\n",
      "Episode 16200\tAvg Score (100): -2.93\tEpsilon: 0.297\n",
      "Episode 16300\tAvg Score (100): -2.93\tEpsilon: 0.294\n",
      "Episode 16400\tAvg Score (100): -2.48\tEpsilon: 0.293\n",
      "Episode 16500\tAvg Score (100): -2.78\tEpsilon: 0.290\n",
      "Episode 16600\tAvg Score (100): -2.64\tEpsilon: 0.288\n",
      "Episode 16700\tAvg Score (100): -2.92\tEpsilon: 0.286\n",
      "Episode 16800\tAvg Score (100): -2.60\tEpsilon: 0.284\n",
      "Episode 16900\tAvg Score (100): -2.54\tEpsilon: 0.281\n",
      "Episode 17000\tAvg Score (100): -2.64\tEpsilon: 0.280\n",
      "Episode 17100\tAvg Score (100): -3.01\tEpsilon: 0.277\n",
      "Episode 17200\tAvg Score (100): -2.49\tEpsilon: 0.276\n",
      "Episode 17300\tAvg Score (100): -2.42\tEpsilon: 0.273\n",
      "Episode 17400\tAvg Score (100): -2.66\tEpsilon: 0.271\n",
      "Episode 17500\tAvg Score (100): -2.71\tEpsilon: 0.269\n",
      "Episode 17600\tAvg Score (100): -2.59\tEpsilon: 0.267\n",
      "Episode 17700\tAvg Score (100): -2.78\tEpsilon: 0.265\n",
      "Episode 17800\tAvg Score (100): -2.43\tEpsilon: 0.263\n",
      "Episode 17900\tAvg Score (100): -2.51\tEpsilon: 0.261\n",
      "Episode 18000\tAvg Score (100): -2.33\tEpsilon: 0.259\n",
      "Episode 18100\tAvg Score (100): -2.32\tEpsilon: 0.257\n",
      "Episode 18200\tAvg Score (100): -2.50\tEpsilon: 0.256\n",
      "Episode 18300\tAvg Score (100): -2.45\tEpsilon: 0.253\n",
      "Episode 18400\tAvg Score (100): -2.42\tEpsilon: 0.252\n",
      "Episode 18500\tAvg Score (100): -2.22\tEpsilon: 0.250\n",
      "Episode 18600\tAvg Score (100): -2.37\tEpsilon: 0.248\n",
      "Episode 18700\tAvg Score (100): -2.23\tEpsilon: 0.246\n",
      "Episode 18800\tAvg Score (100): -2.40\tEpsilon: 0.244\n",
      "Episode 18900\tAvg Score (100): -2.29\tEpsilon: 0.242\n",
      "Episode 19000\tAvg Score (100): -2.20\tEpsilon: 0.241\n",
      "Episode 19100\tAvg Score (100): -2.14\tEpsilon: 0.239\n",
      "Episode 19200\tAvg Score (100): -2.39\tEpsilon: 0.237\n",
      "Episode 19300\tAvg Score (100): -2.28\tEpsilon: 0.235\n",
      "Episode 19400\tAvg Score (100): -2.07\tEpsilon: 0.234\n",
      "Episode 19500\tAvg Score (100): -2.01\tEpsilon: 0.231\n",
      "Episode 19600\tAvg Score (100): -2.08\tEpsilon: 0.230\n",
      "Episode 19700\tAvg Score (100): -1.97\tEpsilon: 0.228\n",
      "Episode 19800\tAvg Score (100): -2.10\tEpsilon: 0.227\n",
      "Episode 19900\tAvg Score (100): -2.39\tEpsilon: 0.225\n",
      "Episode 20000\tAvg Score (100): -2.01\tEpsilon: 0.223\n",
      "Episode 20100\tAvg Score (100): -1.92\tEpsilon: 0.221\n",
      "Episode 20200\tAvg Score (100): -1.56\tEpsilon: 0.220\n",
      "Episode 20300\tAvg Score (100): -2.31\tEpsilon: 0.218\n",
      "Episode 20400\tAvg Score (100): -1.93\tEpsilon: 0.217\n",
      "Episode 20500\tAvg Score (100): -1.72\tEpsilon: 0.215\n",
      "Episode 20600\tAvg Score (100): -1.87\tEpsilon: 0.213\n",
      "Episode 20700\tAvg Score (100): -1.73\tEpsilon: 0.212\n",
      "Episode 20800\tAvg Score (100): -1.68\tEpsilon: 0.210\n",
      "Episode 20900\tAvg Score (100): -1.99\tEpsilon: 0.208\n",
      "Episode 21000\tAvg Score (100): -1.95\tEpsilon: 0.207\n",
      "Episode 21100\tAvg Score (100): -2.18\tEpsilon: 0.205\n",
      "Episode 21200\tAvg Score (100): -2.19\tEpsilon: 0.204\n",
      "Episode 21300\tAvg Score (100): -1.93\tEpsilon: 0.202\n",
      "Episode 21400\tAvg Score (100): -1.93\tEpsilon: 0.201\n",
      "Episode 21500\tAvg Score (100): -1.82\tEpsilon: 0.199\n",
      "Episode 21600\tAvg Score (100): -1.98\tEpsilon: 0.198\n",
      "Episode 21700\tAvg Score (100): -1.59\tEpsilon: 0.196\n",
      "Episode 21800\tAvg Score (100): -1.55\tEpsilon: 0.195\n",
      "Episode 21900\tAvg Score (100): -1.81\tEpsilon: 0.193\n",
      "Episode 22000\tAvg Score (100): -1.70\tEpsilon: 0.192\n",
      "Episode 22100\tAvg Score (100): -1.70\tEpsilon: 0.190\n",
      "Episode 22200\tAvg Score (100): -1.65\tEpsilon: 0.189\n",
      "Episode 22300\tAvg Score (100): -1.77\tEpsilon: 0.188\n",
      "Episode 22400\tAvg Score (100): -1.93\tEpsilon: 0.186\n",
      "Episode 22500\tAvg Score (100): -1.67\tEpsilon: 0.185\n",
      "Episode 22600\tAvg Score (100): -2.01\tEpsilon: 0.184\n",
      "Episode 22700\tAvg Score (100): -1.71\tEpsilon: 0.182\n",
      "Episode 22800\tAvg Score (100): -1.43\tEpsilon: 0.181\n",
      "Episode 22900\tAvg Score (100): -1.46\tEpsilon: 0.179\n",
      "Episode 23000\tAvg Score (100): -1.44\tEpsilon: 0.178\n",
      "Episode 23100\tAvg Score (100): -1.56\tEpsilon: 0.177\n",
      "Episode 23200\tAvg Score (100): -1.49\tEpsilon: 0.176\n",
      "Episode 23300\tAvg Score (100): -1.51\tEpsilon: 0.174\n",
      "Episode 23400\tAvg Score (100): -1.59\tEpsilon: 0.173\n",
      "Episode 23500\tAvg Score (100): -1.45\tEpsilon: 0.171\n",
      "Episode 23600\tAvg Score (100): -1.62\tEpsilon: 0.170\n",
      "Episode 23700\tAvg Score (100): -1.64\tEpsilon: 0.169\n",
      "Episode 23800\tAvg Score (100): -1.37\tEpsilon: 0.168\n",
      "Episode 23900\tAvg Score (100): -1.62\tEpsilon: 0.166\n",
      "Episode 24000\tAvg Score (100): -1.44\tEpsilon: 0.165\n",
      "Episode 24100\tAvg Score (100): -1.46\tEpsilon: 0.164\n",
      "Episode 24200\tAvg Score (100): -1.43\tEpsilon: 0.163\n",
      "Episode 24300\tAvg Score (100): -1.26\tEpsilon: 0.161\n",
      "Episode 24400\tAvg Score (100): -1.54\tEpsilon: 0.160\n",
      "Episode 24500\tAvg Score (100): -1.37\tEpsilon: 0.159\n",
      "Episode 24600\tAvg Score (100): -1.27\tEpsilon: 0.158\n",
      "Episode 24700\tAvg Score (100): -1.35\tEpsilon: 0.157\n",
      "Episode 24800\tAvg Score (100): -1.39\tEpsilon: 0.156\n",
      "Episode 24900\tAvg Score (100): -1.31\tEpsilon: 0.154\n",
      "Episode 25000\tAvg Score (100): -1.47\tEpsilon: 0.153\n",
      "Episode 25100\tAvg Score (100): -1.38\tEpsilon: 0.152\n",
      "Episode 25200\tAvg Score (100): -1.21\tEpsilon: 0.151\n",
      "Episode 25300\tAvg Score (100): -1.35\tEpsilon: 0.150\n",
      "Episode 25400\tAvg Score (100): -1.33\tEpsilon: 0.149\n",
      "Episode 25500\tAvg Score (100): -1.17\tEpsilon: 0.148\n",
      "Episode 25600\tAvg Score (100): -1.18\tEpsilon: 0.147\n",
      "Episode 25700\tAvg Score (100): -1.45\tEpsilon: 0.145\n",
      "Episode 25800\tAvg Score (100): -1.30\tEpsilon: 0.144\n",
      "Episode 25900\tAvg Score (100): -1.21\tEpsilon: 0.143\n",
      "Episode 26000\tAvg Score (100): -1.06\tEpsilon: 0.142\n",
      "Episode 26100\tAvg Score (100): -1.32\tEpsilon: 0.141\n",
      "Episode 26200\tAvg Score (100): -1.14\tEpsilon: 0.140\n",
      "Episode 26300\tAvg Score (100): -1.07\tEpsilon: 0.139\n",
      "Episode 26400\tAvg Score (100): -1.25\tEpsilon: 0.138\n",
      "Episode 26500\tAvg Score (100): -1.36\tEpsilon: 0.137\n",
      "Episode 26600\tAvg Score (100): -1.19\tEpsilon: 0.136\n",
      "Episode 26700\tAvg Score (100): -1.06\tEpsilon: 0.135\n",
      "Episode 26800\tAvg Score (100): -1.05\tEpsilon: 0.134\n",
      "Episode 26900\tAvg Score (100): -1.17\tEpsilon: 0.133\n",
      "Episode 27000\tAvg Score (100): -1.13\tEpsilon: 0.132\n",
      "Episode 27100\tAvg Score (100): -0.95\tEpsilon: 0.131\n",
      "Episode 27200\tAvg Score (100): -1.06\tEpsilon: 0.130\n",
      "Episode 27300\tAvg Score (100): -1.32\tEpsilon: 0.129\n",
      "Episode 27400\tAvg Score (100): -1.22\tEpsilon: 0.128\n",
      "Episode 27500\tAvg Score (100): -1.13\tEpsilon: 0.127\n",
      "Episode 27600\tAvg Score (100): -1.10\tEpsilon: 0.126\n",
      "Episode 27700\tAvg Score (100): -1.28\tEpsilon: 0.125\n",
      "Episode 27800\tAvg Score (100): -0.93\tEpsilon: 0.124\n",
      "Episode 27900\tAvg Score (100): -1.01\tEpsilon: 0.123\n",
      "Episode 28000\tAvg Score (100): -1.13\tEpsilon: 0.122\n",
      "Episode 28100\tAvg Score (100): -1.11\tEpsilon: 0.121\n",
      "Episode 28200\tAvg Score (100): -1.04\tEpsilon: 0.121\n",
      "Episode 28300\tAvg Score (100): -1.21\tEpsilon: 0.120\n",
      "Episode 28400\tAvg Score (100): -0.91\tEpsilon: 0.119\n",
      "Episode 28500\tAvg Score (100): -1.15\tEpsilon: 0.118\n",
      "Episode 28600\tAvg Score (100): -0.94\tEpsilon: 0.117\n",
      "Episode 28700\tAvg Score (100): -1.09\tEpsilon: 0.116\n",
      "Episode 28800\tAvg Score (100): -0.81\tEpsilon: 0.115\n",
      "Episode 28900\tAvg Score (100): -0.88\tEpsilon: 0.114\n",
      "Episode 29000\tAvg Score (100): -1.07\tEpsilon: 0.114\n",
      "Episode 29100\tAvg Score (100): -1.02\tEpsilon: 0.113\n",
      "Episode 29200\tAvg Score (100): -1.01\tEpsilon: 0.112\n",
      "Episode 29300\tAvg Score (100): -1.00\tEpsilon: 0.111\n",
      "Episode 29400\tAvg Score (100): -0.79\tEpsilon: 0.110\n",
      "Episode 29500\tAvg Score (100): -0.75\tEpsilon: 0.109\n",
      "Episode 29600\tAvg Score (100): -1.04\tEpsilon: 0.109\n",
      "Episode 29700\tAvg Score (100): -1.01\tEpsilon: 0.108\n",
      "Episode 29800\tAvg Score (100): -1.02\tEpsilon: 0.107\n",
      "Episode 29900\tAvg Score (100): -0.86\tEpsilon: 0.106\n",
      "Episode 30000\tAvg Score (100): -0.93\tEpsilon: 0.105\n",
      "Episode 30100\tAvg Score (100): -0.88\tEpsilon: 0.104\n",
      "Episode 30200\tAvg Score (100): -0.92\tEpsilon: 0.104\n",
      "Episode 30300\tAvg Score (100): -0.80\tEpsilon: 0.103\n",
      "Episode 30400\tAvg Score (100): -0.91\tEpsilon: 0.102\n",
      "Episode 30500\tAvg Score (100): -0.86\tEpsilon: 0.101\n",
      "Episode 30600\tAvg Score (100): -0.90\tEpsilon: 0.101\n",
      "Episode 30700\tAvg Score (100): -0.93\tEpsilon: 0.100\n",
      "Episode 30800\tAvg Score (100): -0.79\tEpsilon: 0.100\n",
      "Episode 30900\tAvg Score (100): -0.89\tEpsilon: 0.100\n",
      "Episode 31000\tAvg Score (100): -0.83\tEpsilon: 0.100\n",
      "Episode 31100\tAvg Score (100): -0.90\tEpsilon: 0.100\n",
      "Episode 31200\tAvg Score (100): -0.84\tEpsilon: 0.100\n",
      "Episode 31300\tAvg Score (100): -0.98\tEpsilon: 0.100\n",
      "Episode 31400\tAvg Score (100): -0.88\tEpsilon: 0.100\n",
      "Episode 31500\tAvg Score (100): -0.76\tEpsilon: 0.100\n",
      "Episode 31600\tAvg Score (100): -0.77\tEpsilon: 0.100\n",
      "Episode 31700\tAvg Score (100): -1.00\tEpsilon: 0.100\n",
      "Episode 31800\tAvg Score (100): -0.73\tEpsilon: 0.100\n",
      "Episode 31900\tAvg Score (100): -0.83\tEpsilon: 0.100\n",
      "Episode 32000\tAvg Score (100): -0.68\tEpsilon: 0.100\n",
      "Episode 32100\tAvg Score (100): -0.83\tEpsilon: 0.100\n",
      "Episode 32200\tAvg Score (100): -0.98\tEpsilon: 0.100\n",
      "Episode 32300\tAvg Score (100): -0.78\tEpsilon: 0.100\n",
      "Episode 32400\tAvg Score (100): -0.71\tEpsilon: 0.100\n",
      "Episode 32500\tAvg Score (100): -0.82\tEpsilon: 0.100\n",
      "Episode 32600\tAvg Score (100): -0.87\tEpsilon: 0.100\n",
      "Episode 32700\tAvg Score (100): -0.71\tEpsilon: 0.100\n",
      "Episode 32800\tAvg Score (100): -0.81\tEpsilon: 0.100\n",
      "Episode 32900\tAvg Score (100): -0.94\tEpsilon: 0.100\n",
      "Episode 33000\tAvg Score (100): -0.76\tEpsilon: 0.100\n",
      "Episode 33100\tAvg Score (100): -0.82\tEpsilon: 0.100\n",
      "Episode 33200\tAvg Score (100): -0.81\tEpsilon: 0.100\n",
      "Episode 33300\tAvg Score (100): -0.88\tEpsilon: 0.100\n",
      "Episode 33400\tAvg Score (100): -0.81\tEpsilon: 0.100\n",
      "Episode 33500\tAvg Score (100): -0.87\tEpsilon: 0.100\n",
      "Episode 33600\tAvg Score (100): -0.83\tEpsilon: 0.100\n",
      "Episode 33700\tAvg Score (100): -1.00\tEpsilon: 0.100\n",
      "Episode 33800\tAvg Score (100): -0.71\tEpsilon: 0.100\n",
      "Episode 33900\tAvg Score (100): -0.84\tEpsilon: 0.100\n",
      "Episode 34000\tAvg Score (100): -0.90\tEpsilon: 0.100\n",
      "Episode 34100\tAvg Score (100): -0.92\tEpsilon: 0.100\n",
      "Episode 34200\tAvg Score (100): -0.82\tEpsilon: 0.100\n",
      "Episode 34300\tAvg Score (100): -0.87\tEpsilon: 0.100\n",
      "Episode 34400\tAvg Score (100): -0.90\tEpsilon: 0.100\n",
      "Episode 34500\tAvg Score (100): -0.74\tEpsilon: 0.100\n",
      "Episode 34600\tAvg Score (100): -0.98\tEpsilon: 0.100\n",
      "Episode 34700\tAvg Score (100): -0.76\tEpsilon: 0.100\n",
      "Episode 34800\tAvg Score (100): -0.77\tEpsilon: 0.100\n",
      "Episode 34900\tAvg Score (100): -0.89\tEpsilon: 0.100\n",
      "Episode 35000\tAvg Score (100): -1.04\tEpsilon: 0.100\n",
      "Episode 35100\tAvg Score (100): -0.86\tEpsilon: 0.100\n",
      "Episode 35200\tAvg Score (100): -0.81\tEpsilon: 0.100\n",
      "Episode 35300\tAvg Score (100): -0.92\tEpsilon: 0.100\n",
      "Episode 35400\tAvg Score (100): -0.75\tEpsilon: 0.100\n",
      "Episode 35500\tAvg Score (100): -1.02\tEpsilon: 0.100\n",
      "Episode 35600\tAvg Score (100): -0.78\tEpsilon: 0.100\n",
      "Episode 35700\tAvg Score (100): -0.62\tEpsilon: 0.100\n",
      "Episode 35800\tAvg Score (100): -1.01\tEpsilon: 0.100\n",
      "Episode 35900\tAvg Score (100): -0.77\tEpsilon: 0.100\n",
      "Episode 36000\tAvg Score (100): -1.05\tEpsilon: 0.100\n",
      "Episode 36100\tAvg Score (100): -0.77\tEpsilon: 0.100\n",
      "Episode 36200\tAvg Score (100): -0.62\tEpsilon: 0.100\n",
      "Episode 36300\tAvg Score (100): -0.92\tEpsilon: 0.100\n",
      "Episode 36400\tAvg Score (100): -0.85\tEpsilon: 0.100\n",
      "Episode 36500\tAvg Score (100): -0.85\tEpsilon: 0.100\n",
      "Episode 36600\tAvg Score (100): -0.67\tEpsilon: 0.100\n",
      "Episode 36700\tAvg Score (100): -0.77\tEpsilon: 0.100\n",
      "Episode 36800\tAvg Score (100): -0.89\tEpsilon: 0.100\n",
      "Episode 36900\tAvg Score (100): -0.86\tEpsilon: 0.100\n",
      "Episode 37000\tAvg Score (100): -0.82\tEpsilon: 0.100\n",
      "Episode 37100\tAvg Score (100): -0.73\tEpsilon: 0.100\n",
      "Episode 37200\tAvg Score (100): -0.92\tEpsilon: 0.100\n",
      "Episode 37300\tAvg Score (100): -0.96\tEpsilon: 0.100\n",
      "Episode 37400\tAvg Score (100): -1.18\tEpsilon: 0.100\n",
      "Episode 37500\tAvg Score (100): -0.82\tEpsilon: 0.100\n",
      "Episode 37600\tAvg Score (100): -0.85\tEpsilon: 0.100\n",
      "Episode 37700\tAvg Score (100): -0.77\tEpsilon: 0.100\n",
      "Episode 37800\tAvg Score (100): -1.02\tEpsilon: 0.100\n",
      "Episode 37900\tAvg Score (100): -0.86\tEpsilon: 0.100\n",
      "Episode 38000\tAvg Score (100): -0.77\tEpsilon: 0.100\n",
      "Episode 38100\tAvg Score (100): -0.80\tEpsilon: 0.100\n",
      "Episode 38200\tAvg Score (100): -0.73\tEpsilon: 0.100\n",
      "Episode 38300\tAvg Score (100): -0.85\tEpsilon: 0.100\n",
      "Episode 38400\tAvg Score (100): -0.71\tEpsilon: 0.100\n",
      "Episode 38500\tAvg Score (100): -0.65\tEpsilon: 0.100\n",
      "Episode 38600\tAvg Score (100): -0.77\tEpsilon: 0.100\n",
      "Episode 38700\tAvg Score (100): -0.85\tEpsilon: 0.100\n",
      "Episode 38800\tAvg Score (100): -1.01\tEpsilon: 0.100\n",
      "Episode 38900\tAvg Score (100): -0.88\tEpsilon: 0.100\n",
      "Episode 39000\tAvg Score (100): -0.87\tEpsilon: 0.100\n",
      "Episode 39100\tAvg Score (100): -0.90\tEpsilon: 0.100\n",
      "Episode 39200\tAvg Score (100): -0.75\tEpsilon: 0.100\n",
      "Episode 39300\tAvg Score (100): -0.74\tEpsilon: 0.100\n",
      "Episode 39400\tAvg Score (100): -0.69\tEpsilon: 0.100\n",
      "Episode 39500\tAvg Score (100): -0.84\tEpsilon: 0.100\n",
      "Episode 39600\tAvg Score (100): -0.80\tEpsilon: 0.100\n",
      "Episode 39700\tAvg Score (100): -0.84\tEpsilon: 0.100\n",
      "Episode 39800\tAvg Score (100): -0.87\tEpsilon: 0.100\n",
      "Episode 39900\tAvg Score (100): -0.84\tEpsilon: 0.100\n",
      "Episode 40000\tAvg Score (100): -0.87\tEpsilon: 0.100\n",
      "Episode 40100\tAvg Score (100): -0.95\tEpsilon: 0.100\n",
      "Episode 40200\tAvg Score (100): -0.79\tEpsilon: 0.100\n",
      "Episode 40300\tAvg Score (100): -0.71\tEpsilon: 0.100\n",
      "Episode 40400\tAvg Score (100): -0.93\tEpsilon: 0.100\n",
      "Episode 40500\tAvg Score (100): -0.73\tEpsilon: 0.100\n",
      "Episode 40600\tAvg Score (100): -0.73\tEpsilon: 0.100\n",
      "Episode 40700\tAvg Score (100): -0.91\tEpsilon: 0.100\n",
      "Episode 40800\tAvg Score (100): -0.71\tEpsilon: 0.100\n",
      "Episode 40900\tAvg Score (100): -0.90\tEpsilon: 0.100\n",
      "Episode 41000\tAvg Score (100): -0.84\tEpsilon: 0.100\n",
      "Episode 41100\tAvg Score (100): -0.87\tEpsilon: 0.100\n",
      "Episode 41200\tAvg Score (100): -1.01\tEpsilon: 0.100\n",
      "Episode 41300\tAvg Score (100): -0.83\tEpsilon: 0.100\n",
      "Episode 41400\tAvg Score (100): -0.82\tEpsilon: 0.100\n",
      "Episode 41500\tAvg Score (100): -0.79\tEpsilon: 0.100\n",
      "Episode 41600\tAvg Score (100): -0.68\tEpsilon: 0.100\n",
      "Episode 41700\tAvg Score (100): -0.89\tEpsilon: 0.100\n",
      "Episode 41800\tAvg Score (100): -0.74\tEpsilon: 0.100\n",
      "Episode 41900\tAvg Score (100): -0.91\tEpsilon: 0.100\n",
      "Episode 42000\tAvg Score (100): -0.93\tEpsilon: 0.100\n",
      "Episode 42100\tAvg Score (100): -0.72\tEpsilon: 0.100\n",
      "Episode 42200\tAvg Score (100): -0.78\tEpsilon: 0.100\n",
      "Episode 42300\tAvg Score (100): -0.80\tEpsilon: 0.100\n",
      "Episode 42400\tAvg Score (100): -0.89\tEpsilon: 0.100\n",
      "Episode 42500\tAvg Score (100): -0.86\tEpsilon: 0.100\n",
      "Episode 42600\tAvg Score (100): -0.77\tEpsilon: 0.100\n",
      "Episode 42700\tAvg Score (100): -0.80\tEpsilon: 0.100\n",
      "Episode 42800\tAvg Score (100): -0.73\tEpsilon: 0.100\n",
      "Episode 42900\tAvg Score (100): -0.82\tEpsilon: 0.100\n",
      "Episode 43000\tAvg Score (100): -0.82\tEpsilon: 0.100\n",
      "Episode 43100\tAvg Score (100): -0.92\tEpsilon: 0.100\n",
      "Episode 43200\tAvg Score (100): -0.82\tEpsilon: 0.100\n",
      "Episode 43300\tAvg Score (100): -0.69\tEpsilon: 0.100\n",
      "Episode 43400\tAvg Score (100): -0.89\tEpsilon: 0.100\n",
      "Episode 43500\tAvg Score (100): -0.86\tEpsilon: 0.100\n",
      "Episode 43600\tAvg Score (100): -0.83\tEpsilon: 0.100\n",
      "Episode 43700\tAvg Score (100): -1.01\tEpsilon: 0.100\n",
      "Episode 43800\tAvg Score (100): -0.84\tEpsilon: 0.100\n",
      "Episode 43900\tAvg Score (100): -0.78\tEpsilon: 0.100\n",
      "Episode 44000\tAvg Score (100): -0.70\tEpsilon: 0.100\n",
      "Episode 44100\tAvg Score (100): -0.81\tEpsilon: 0.100\n",
      "Episode 44200\tAvg Score (100): -0.86\tEpsilon: 0.100\n",
      "Episode 44300\tAvg Score (100): -0.76\tEpsilon: 0.100\n",
      "Episode 44400\tAvg Score (100): -0.96\tEpsilon: 0.100\n",
      "Episode 44500\tAvg Score (100): -0.96\tEpsilon: 0.100\n",
      "Episode 44600\tAvg Score (100): -0.64\tEpsilon: 0.100\n",
      "Episode 44700\tAvg Score (100): -0.72\tEpsilon: 0.100\n",
      "Episode 44800\tAvg Score (100): -0.82\tEpsilon: 0.100\n",
      "Episode 44900\tAvg Score (100): -0.94\tEpsilon: 0.100\n",
      "Episode 45000\tAvg Score (100): -0.96\tEpsilon: 0.100\n",
      "--- Training Complete ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_25776\\3780510772.py:471: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  agent.policy_net.load_state_dict(torch.load(\"dqn_portfolio_model_highscore.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Evaluation ---\n",
      "Loaded 'dqn_portfolio_model_highscore.pth' for evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final standard deviation is zero.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from collections import deque\n",
    "\n",
    "# from scipy.stats import mode\n",
    "import sys\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "\n",
    "from or_gym.envs.finance.discrete_portfolio_opt import DiscretePortfolioOptEnv\n",
    "\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "matplotlib.use('tkagg')\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "device=\"cpu\"\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class PolicyModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions,n_classes):\n",
    "        super(PolicyModel, self).__init__()\n",
    "\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    " \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class TargetModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions,n_classes):\n",
    "        super(TargetModel, self).__init__()\n",
    "\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    " \n",
    "        return x\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "from itertools import product\n",
    "# current best at LR = 2e-4 and original network\n",
    "# --- Hyperparameters ---\n",
    "BUFFER_SIZE = 100_000   # Replay buffer size\n",
    "BATCH_SIZE = 64         # Mini-batch size\n",
    "GAMMA = 0.98             \n",
    "LR = 3e-4               # Learning rate\n",
    "# MIN_LR = 1e-6\n",
    "TAU = 1e-2              # For soft target network updates\n",
    "TARGET_UPDATE_STYLE = 'hard'\n",
    "NUM_EPISODES = 45000     \n",
    "EPS_START = 1.0         \n",
    "EPS_END = 0.1          \n",
    "EPS_DECAY = 0.997      \n",
    "TARGET_UPDATE_FREQ = 1000  # For hard target updates\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "# class QNetwork(nn.Module):\n",
    "#     def __init__(self, obs_dim, num_actions):\n",
    "#         super(QNetwork, self).__init__()\n",
    "        \n",
    "#         self.network = nn.Sequential(\n",
    "#             nn.Linear(obs_dim, 256),\n",
    "#             nn.LayerNorm(256),\n",
    "#             # nn.Dropout(p=0.2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, 1024),\n",
    "#             nn.LayerNorm(1024),\n",
    "#             # nn.Dropout(p=0.3),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(1024, num_actions)\n",
    "#         )\n",
    "\n",
    "#         # apply Kaiming init to hidden layers\n",
    "#         self.network.apply(init_weights)\n",
    "\n",
    "#         # small uniform init for final (output) layer\n",
    "#         nn.init.uniform_(self.network[-1].weight, -1e-3, 1e-3)\n",
    "#         nn.init.constant_(self.network[-1].bias, 0.0)\n",
    "\n",
    "#     def forward(self, state):\n",
    "#         return self.network(state)\n",
    "    \n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, num_actions):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        # Shared MLP body\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1024),\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # --- Dueling Streams ---\n",
    "        # 1. Value Stream (computes V(s))\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1) # Single output for state value\n",
    "        )\n",
    "        \n",
    "        # 2. Advantage Stream (computes A(s,a))\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions) # 3125 outputs for action advantages\n",
    "        )\n",
    "\n",
    "        # Apply init (we apply to each new part)\n",
    "        self.body.apply(init_weights)\n",
    "        self.value_stream.apply(init_weights)\n",
    "        self.advantage_stream.apply(init_weights)\n",
    "        \n",
    "        # Small uniform init for final advantage layer\n",
    "        nn.init.uniform_(self.advantage_stream[-1].weight, -1e-3, 1e-3)\n",
    "        nn.init.constant_(self.advantage_stream[-1].bias, 0.0)\n",
    "\n",
    "    def forward(self, state):\n",
    "        # Pass state through the shared body\n",
    "        shared_embedding = self.body(state)\n",
    "        \n",
    "        # Get V(s) and A(s,a)\n",
    "        value = self.value_stream(shared_embedding)\n",
    "        advantages = self.advantage_stream(shared_embedding)\n",
    "        \n",
    "        # --- Recombine V and A to get Q(s,a) ---\n",
    "        # Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))\n",
    "        q_values = value + (advantages - advantages.mean(dim=1, keepdim=True))\n",
    "        return q_values\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# --- 1. Simplified 1-Step Replay Buffer ---\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, device):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.device = device\n",
    "        \n",
    "        # Simple 1-step experience\n",
    "        self.experience = namedtuple(\"Experience\", \n",
    "                                     field_names=[\"state\", \"action_index\", \"reward\", \n",
    "                                                  \"next_state\", \"done\"])\n",
    "\n",
    "    def push(self, state, action_index, reward, next_state, done):\n",
    "        \"\"\"Adds a 1-step experience to memory.\"\"\"\n",
    "        e = self.experience(state, action_index, reward, next_state, done)\n",
    "        self.buffer.append(e)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        experiences = random.sample(self.buffer, k=batch_size)\n",
    "        \n",
    "        states = torch.tensor(\n",
    "            np.vstack([e.state for e in experiences if e is not None]), \n",
    "            dtype=torch.float32).to(self.device)\n",
    "        action_indices = torch.tensor(\n",
    "            np.vstack([e.action_index for e in experiences if e is not None]), \n",
    "            dtype=torch.int64).to(self.device)\n",
    "        rewards = torch.tensor(\n",
    "            np.vstack([e.reward for e in experiences if e is not None]), \n",
    "            dtype=torch.float32).to(self.device)\n",
    "        next_states = torch.tensor(\n",
    "            np.vstack([e.next_state for e in experiences if e is not None]), \n",
    "            dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(\n",
    "            np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8), \n",
    "            dtype=torch.float32).to(self.device)\n",
    "\n",
    "        return (states, action_indices, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class StateManager:\n",
    "    \"\"\"\n",
    "    Handles feature engineering outside the environment.\n",
    "    It takes the raw state from the env and computes the 27-dim state.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_assets, step_limit):\n",
    "        self.num_assets = num_assets\n",
    "        self.step_limit = step_limit\n",
    "        self.price_history = np.zeros((self.num_assets, self.step_limit + 1))\n",
    "        self.time_step = 0\n",
    "\n",
    "    def reset(self, raw_initial_state):\n",
    "        \"\"\"\n",
    "        Resets the history and computes the initial engineered state.\n",
    "        raw_initial_state = [cash, price1..5, shares1..5, time]\n",
    "        \"\"\"\n",
    "        self.time_step = 0\n",
    "        self.price_history = np.zeros((self.num_assets, self.step_limit + 1))\n",
    "        \n",
    "        # Extract initial prices\n",
    "        initial_prices = raw_initial_state[1 : 1 + self.num_assets]\n",
    "        self.price_history[:, 0] = initial_prices\n",
    "        \n",
    "        # Return the feature-engineered state for t=0\n",
    "        return self._compute_features(raw_initial_state)\n",
    "\n",
    "    def process_state(self, raw_state):\n",
    "        \"\"\"\n",
    "        Computes the engineered state for the current timestep.\n",
    "        raw_state = [cash, price1..5, shares1..5, time]\n",
    "        \"\"\"\n",
    "        self.time_step = int(raw_state[-1]) # Get time from the raw state\n",
    "        \n",
    "        # Store current prices in our history\n",
    "        current_prices = raw_state[1 : 1 + self.num_assets]\n",
    "        if self.time_step <= self.step_limit:\n",
    "            self.price_history[:, self.time_step] = current_prices\n",
    "        \n",
    "        return self._compute_features(raw_state)\n",
    "\n",
    "    def _get_price(self, t):\n",
    "        \"\"\"Helper to safely get price at time t.\"\"\"\n",
    "        if t < 0:\n",
    "            return self.price_history[:, 0] # Repeat initial price for t<0\n",
    "        return self.price_history[:, t]\n",
    "\n",
    "    def _compute_features(self, raw_state):\n",
    "        \"\"\"Calculates the 27-dimensional feature-engineered state.\"\"\"\n",
    "        \n",
    "        t = self.time_step\n",
    "        \n",
    "        # Extract data from raw state\n",
    "        cash = raw_state[0]\n",
    "        current_prices = raw_state[1 : 1 + self.num_assets]\n",
    "        holdings = raw_state[1 + self.num_assets : 1 + 2 * self.num_assets]\n",
    "\n",
    "        # 1. Current Price (5 features)\n",
    "        # (already have current_prices)\n",
    "        \n",
    "        # 2. Change in price (t-1) -> (t) (5 features)\n",
    "        price_t_minus_1 = self._get_price(t - 1)\n",
    "        change_t_1 = current_prices - price_t_minus_1\n",
    "        \n",
    "        # 3. Change in price (t-2) -> (t-1) (5 features)\n",
    "        price_t_minus_2 = self._get_price(t - 2)\n",
    "        change_t_2 = price_t_minus_1 - price_t_minus_2\n",
    "        \n",
    "        # 4. Diff from mean (5 features)\n",
    "        episode_prices = self.price_history[:, :t + 1]\n",
    "        mean_price = np.mean(episode_prices, axis=1)\n",
    "        diff_from_mean = current_prices - mean_price\n",
    "        \n",
    "        # 5. Current Holdings (5 features)\n",
    "        # (already have holdings)\n",
    "        \n",
    "        # 6. Time and Cash (2 features)\n",
    "        time_feature = np.array([t / self.step_limit]) \n",
    "        cash_feature = np.array([cash])\n",
    "        \n",
    "        # Concatenate all 27 features\n",
    "        engineered_state = np.concatenate([\n",
    "            current_prices,\n",
    "            change_t_1,\n",
    "            change_t_2,\n",
    "            diff_from_mean,\n",
    "            holdings,\n",
    "            time_feature,\n",
    "            cash_feature\n",
    "        ]).astype(np.float32)\n",
    "        \n",
    "        return engineered_state\n",
    "\n",
    "# --- 2. DQNAgent (1-Step Update) ---\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, engineered_obs_dim):\n",
    "        self.env = env\n",
    "        self.obs_dim = engineered_obs_dim \n",
    "        self.num_assets = env.num_assets\n",
    "        self.lot_size = env.lot_size\n",
    "        \n",
    "        self.num_actions_per_asset = (2 * self.lot_size) + 1 \n",
    "        self.total_actions = self.num_actions_per_asset ** self.num_assets \n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"--- Agent Initialized (1-Step DQN) ---\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        print(f\"Engineered State Dim: {self.obs_dim}\")\n",
    "        \n",
    "        self.gamma = GAMMA \n",
    "\n",
    "        self.policy_net = QNetwork(self.obs_dim, self.total_actions).to(self.device)\n",
    "        self.target_net = QNetwork(self.obs_dim, self.total_actions).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=LR)\n",
    "        self.loss_fn = nn.SmoothL1Loss()\n",
    "        \n",
    "        # Simplified Buffer Initialization\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, self.device)\n",
    "        \n",
    "        self.steps_done = 0\n",
    "        self.epsilon = EPS_START\n",
    "\n",
    "    # ... (Action mapping helpers and select_action are UNCHANGED) ...\n",
    "    def _map_index_to_vector(self, index):\n",
    "        action_vector = []\n",
    "        temp_index = index\n",
    "        base = self.num_actions_per_asset\n",
    "        for i in range(self.num_assets):\n",
    "            action_0_to_4 = temp_index % base\n",
    "            action_minus_2_to_2 = action_0_to_4 - self.lot_size\n",
    "            action_vector.append(action_minus_2_to_2)\n",
    "            temp_index //= base\n",
    "        return np.array(action_vector)\n",
    "\n",
    "    def _map_vector_to_index(self, action_vector):\n",
    "        index = 0\n",
    "        base = self.num_actions_per_asset\n",
    "        for i in range(self.num_assets):\n",
    "            action_0_to_4 = action_vector[i] + self.lot_size\n",
    "            index += action_0_to_4 * (base ** i)\n",
    "        return int(index)\n",
    "\n",
    "    def select_action(self, state, greedy=False):\n",
    "            # Epsilon decay\n",
    "            if not greedy:\n",
    "                self.epsilon = max(EPS_END, EPS_START * EPS_DECAY**(self.steps_done//400.0))\n",
    "                self.steps_done += 1\n",
    "            \n",
    "            # Select action\n",
    "            if not greedy and random.random() < self.epsilon:\n",
    "                # --- Exploration ---\n",
    "                action_index = random.randrange(self.total_actions)\n",
    "            else:\n",
    "                # --- Exploitation (or greedy evaluation) ---\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    q_values = self.policy_net(state_tensor)\n",
    "                action_index = q_values.argmax().item()\n",
    "            \n",
    "            action_vector = self._map_index_to_vector(action_index)\n",
    "            return action_vector, action_index\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        # Sample 1-step transitions\n",
    "        states, action_indices, rewards, next_states, dones = self.memory.sample(BATCH_SIZE)\n",
    "        \n",
    "        # --- 1. Calculate 1-Step DDQN Target ---\n",
    "        with torch.no_grad():\n",
    "            next_action_indices = self.policy_net(next_states).argmax(dim=1).unsqueeze(1)\n",
    "            # Evaluate those actions with *target_net*\n",
    "            next_q_values = self.target_net(next_states).gather(1, next_action_indices)\n",
    "            \n",
    "            # Standard 1-step Bellman equation\n",
    "            target_q_values = rewards + ( self.gamma * next_q_values * (1 - dones) )\n",
    "\n",
    "        # --- 2. Calculate Current Q-Values ---\n",
    "        current_q_values_all = self.policy_net(states)\n",
    "        current_q_values = current_q_values_all.gather(1, action_indices)\n",
    "\n",
    "        # --- 3. Compute Loss ---\n",
    "        loss = self.loss_fn(current_q_values, target_q_values)\n",
    "        \n",
    "        # --- 4. Optimize ---\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 24.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if TARGET_UPDATE_STYLE == 'soft':\n",
    "            for target_param, policy_param in zip(self.target_net.parameters(), self.policy_net.parameters()):\n",
    "                target_param.data.copy_(TAU * policy_param.data + (1.0 - TAU) * target_param.data)\n",
    "        elif self.steps_done % TARGET_UPDATE_FREQ == 0:\n",
    "             self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "             \n",
    "        return loss.item()\n",
    "\n",
    "def plot_loss(loss_history):\n",
    "    \"\"\"Plots the training loss over optimization steps.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(loss_history)\n",
    "    plt.title(\"Training Loss vs. Optimization Steps\")\n",
    "    plt.xlabel(\"Optimization Steps\")\n",
    "    plt.ylabel(\"Smooth L1 Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def get_portfolio_value(raw_state, num_assets):\n",
    "    \"\"\"Helper to calculate portfolio value from a raw state.\"\"\"\n",
    "    cash = raw_state[0]\n",
    "    prices = raw_state[1 : 1 + num_assets]\n",
    "    holdings = raw_state[1 + num_assets : 1 + 2 * num_assets]\n",
    "    return cash + np.dot(prices, holdings)\n",
    "\n",
    "def evaluate_model(agent, env, manager, num_seeds=100):\n",
    "    \"\"\"Evaluates the trained agent over 100 seeds.\"\"\"\n",
    "    print(\"\\n--- Starting Evaluation ---\")\n",
    "    \n",
    "    # Try to load the high-score model\n",
    "    try:\n",
    "        agent.policy_net.load_state_dict(torch.load(\"dqn_portfolio_model_highscore.pth\"))\n",
    "        print(\"Loaded 'dqn_portfolio_model_highscore.pth' for evaluation.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Evaluation model not found. Using the model from end of training.\")\n",
    "        \n",
    "    agent.policy_net.eval() # Set model to evaluation mode\n",
    "    \n",
    "    num_steps = env.step_limit\n",
    "    num_assets = env.num_assets\n",
    "    \n",
    "    # Store wealth for all seeds and all timesteps\n",
    "    # (seeds, timesteps + 1) -> +1 for initial wealth\n",
    "    all_wealths = np.zeros((num_seeds, num_steps + 1))\n",
    "    \n",
    "    for i in trange(num_seeds): # Use trange for a progress bar\n",
    "        raw_state = env.reset() # Assuming env.reset() returns (state, info)\n",
    "        state = manager.reset(raw_state)\n",
    "        \n",
    "        all_wealths[i, 0] = env.initial_cash\n",
    "        \n",
    "        for t in range(num_steps):\n",
    "            # Use greedy action selection (no exploration)\n",
    "            action_vector, _ = agent.select_action(state, greedy=True)\n",
    "            \n",
    "            next_raw_state, reward, done, _ = env.step(action_vector)\n",
    "            next_state = manager.process_state(next_raw_state)\n",
    "            \n",
    "            state = next_state\n",
    "            raw_state = next_raw_state\n",
    "            \n",
    "            # Calculate and store current portfolio value\n",
    "            current_value = get_portfolio_value(raw_state, num_assets)\n",
    "            all_wealths[i, t + 1] = current_value\n",
    "            \n",
    "            if done:\n",
    "                # If done early, fill remaining steps with last value\n",
    "                all_wealths[i, t+2:] = current_value\n",
    "                break      \n",
    "    return all_wealths\n",
    "\n",
    "def plot_evaluation(all_wealths):\n",
    "    \"\"\"Plots the mean wealth and std deviation from evaluation.\"\"\"\n",
    "    mean_wealth = np.mean(all_wealths, axis=0)\n",
    "    std_wealth = np.std(all_wealths, axis=0)\n",
    "    \n",
    "    timesteps = np.arange(len(mean_wealth))\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(timesteps, mean_wealth, label=\"Mean Portfolio Wealth\", color=\"blue\", lw=2)\n",
    "    \n",
    "    # Create the shaded standard deviation area\n",
    "    plt.fill_between(timesteps, \n",
    "                     mean_wealth - std_wealth, \n",
    "                     mean_wealth + std_wealth, \n",
    "                     color=\"blue\", alpha=0.2, label=\"Std. Deviation\")\n",
    "    \n",
    "    plt.title(f\"Portfolio Wealth Over 100 Seeds (Dueling DDQN)\")\n",
    "    plt.xlabel(\"Timestep\")\n",
    "    plt.ylabel(\"Portfolio Wealth ($)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Report final ratio\n",
    "    mean_final_wealth = mean_wealth[-1]\n",
    "    std_final_wealth = std_wealth[-1]\n",
    "    \n",
    "    if std_final_wealth > 0:\n",
    "        ratio = mean_final_wealth / std_final_wealth\n",
    "        print(f\"\\n--- Evaluation Report ---\")\n",
    "        print(f\"Mean Final Wealth: {mean_final_wealth:.2f}\")\n",
    "        print(f\"Std. Dev Final Wealth: {std_final_wealth:.2f}\")\n",
    "        print(f\"Mean/Std. Dev Ratio: {ratio:.2f}\")\n",
    "    else:\n",
    "        print(\"Final standard deviation is zero.\")\n",
    "             \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    env = DiscretePortfolioOptEnv() \n",
    "\n",
    "    manager = StateManager(env.num_assets, env.step_limit)\n",
    "    _temp_raw_state = env.reset()\n",
    "    ENGINEERED_STATE_DIM = manager.reset(_temp_raw_state).shape[0] \n",
    "    \n",
    "    agent = DQNAgent(env, engineered_obs_dim=ENGINEERED_STATE_DIM)\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(agent.optimizer, gamma=0.999)\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    loss_history = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    \n",
    "    for i_episode in range(1, NUM_EPISODES + 1):\n",
    "        raw_state = env.reset()\n",
    "        state = manager.reset(raw_state) \n",
    "        \n",
    "        episode_score = 0\n",
    "        for t in range(env.step_limit ): \n",
    "            action_vector, action_index = agent.select_action(state)\n",
    "            if t >= env.step_limit:\n",
    "                next_raw_state, reward, done = raw_state, 0.0, True\n",
    "            else:\n",
    "                next_raw_state, reward, done, _ = env.step(action_vector)\n",
    "            \n",
    "            next_state = manager.process_state(next_raw_state)\n",
    "            agent.memory.push(state, action_index, reward, next_state, done)\n",
    "            state = next_state\n",
    "            raw_state = next_raw_state\n",
    "            \n",
    "            if not (t >= env.step_limit):\n",
    "                episode_score += reward\n",
    "            \n",
    "            loss = agent.learn()\n",
    "            # agent.learn()\n",
    "            if loss is not None:\n",
    "                loss_history.append(loss)\n",
    "            \n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # scheduler.step()\n",
    "        scores_window.append(episode_score)\n",
    "        scores.append(episode_score)\n",
    "        \n",
    "        if i_episode % 100 == 0:\n",
    "            avg_score = np.mean(scores_window)\n",
    "            # elapsed_time = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start_time))\n",
    "            print(f\"Episode {i_episode}\\tAvg Score (100): {avg_score:.2f}\\tEpsilon: {agent.epsilon:.3f}\")\n",
    "            if avg_score >= 10.0:\n",
    "                print(f\"\\n--- Environment Good in {i_episode} episodes! ---\")\n",
    "                torch.save(agent.policy_net.state_dict(), \"dqn_portfolio_model.pth\")\n",
    "            if avg_score >= 18.0:\n",
    "                print(f\"\\n--- Environment Solved in {i_episode} episodes! ---\")\n",
    "                torch.save(agent.policy_net.state_dict(), \"dqn_portfolio_model_highscore.pth\")\n",
    "                break\n",
    "        \n",
    "    torch.save(agent.policy_net.state_dict(), \"dqn_portfolio_model_highscore.pth\")       \n",
    "    # env.close() \n",
    "    print(\"--- Training Complete ---\")\n",
    "    \n",
    "    plot_loss(loss_history)\n",
    "    \n",
    "    # Run evaluation\n",
    "    evaluation_wealths = evaluate_model(agent, env, manager, num_seeds=100)\n",
    "    \n",
    "    # Plot evaluation\n",
    "    plot_evaluation(evaluation_wealths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d02b7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Evaluation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_28184\\766624780.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  agent.policy_net.load_state_dict(torch.load(\"dqn_portfolio_model.pth\"))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for QNetwork:\n\tMissing key(s) in state_dict: \"network.0.weight\", \"network.0.bias\", \"network.1.weight\", \"network.1.bias\", \"network.3.weight\", \"network.3.bias\", \"network.4.weight\", \"network.4.bias\", \"network.6.weight\", \"network.6.bias\". \n\tUnexpected key(s) in state_dict: \"body.0.weight\", \"body.0.bias\", \"body.1.weight\", \"body.1.bias\", \"body.3.weight\", \"body.3.bias\", \"body.4.weight\", \"body.4.bias\", \"value_stream.0.weight\", \"value_stream.0.bias\", \"value_stream.2.weight\", \"value_stream.2.bias\", \"advantage_stream.0.weight\", \"advantage_stream.0.bias\", \"advantage_stream.2.weight\", \"advantage_stream.2.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m      \n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_wealths\n\u001b[1;32m---> 47\u001b[0m evaluation_wealths \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# Plot evaluation\u001b[39;00m\n\u001b[0;32m     50\u001b[0m plot_evaluation(evaluation_wealths)\n",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(agent, env, manager, num_seeds)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Try to load the high-score model\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 7\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdqn_portfolio_model.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdqn_portfolio_model_highscore.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for evaluation.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\adity\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2580\u001b[0m             ),\n\u001b[0;32m   2581\u001b[0m         )\n\u001b[0;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2587\u001b[0m         )\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for QNetwork:\n\tMissing key(s) in state_dict: \"network.0.weight\", \"network.0.bias\", \"network.1.weight\", \"network.1.bias\", \"network.3.weight\", \"network.3.bias\", \"network.4.weight\", \"network.4.bias\", \"network.6.weight\", \"network.6.bias\". \n\tUnexpected key(s) in state_dict: \"body.0.weight\", \"body.0.bias\", \"body.1.weight\", \"body.1.bias\", \"body.3.weight\", \"body.3.bias\", \"body.4.weight\", \"body.4.bias\", \"value_stream.0.weight\", \"value_stream.0.bias\", \"value_stream.2.weight\", \"value_stream.2.bias\", \"advantage_stream.0.weight\", \"advantage_stream.0.bias\", \"advantage_stream.2.weight\", \"advantage_stream.2.bias\". "
     ]
    }
   ],
   "source": [
    "def evaluate_model(agent, env, manager, num_seeds=500):\n",
    "    \"\"\"Evaluates the trained agent over 100 seeds.\"\"\"\n",
    "    print(\"\\n--- Starting Evaluation ---\")\n",
    "    \n",
    "    # Try to load the high-score model\n",
    "    try:\n",
    "        agent.policy_net.load_state_dict(torch.load(\"dqn_portfolio_model.pth\"))\n",
    "        print(\"Loaded 'dqn_portfolio_model_highscore.pth' for evaluation.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Evaluation model not found. Using the model from end of training.\")\n",
    "        \n",
    "    agent.policy_net.eval() # Set model to evaluation mode\n",
    "    \n",
    "    num_steps = env.step_limit\n",
    "    num_assets = env.num_assets\n",
    "    \n",
    "    # Store wealth for all seeds and all timesteps\n",
    "    # (seeds, timesteps + 1) -> +1 for initial wealth\n",
    "    all_wealths = np.zeros((num_seeds, num_steps + 1))\n",
    "    \n",
    "    for i in trange(num_seeds): # Use trange for a progress bar\n",
    "        raw_state = env.reset() # Assuming env.reset() returns (state, info)\n",
    "        state = manager.reset(raw_state)\n",
    "        \n",
    "        all_wealths[i, 0] = env.initial_cash\n",
    "        \n",
    "        for t in range(num_steps):\n",
    "            # Use greedy action selection (no exploration)\n",
    "            action_vector, _ = agent.select_action(state, greedy=True)\n",
    "            \n",
    "            next_raw_state, reward, done, _ = env.step(action_vector)\n",
    "            next_state = manager.process_state(next_raw_state)\n",
    "            \n",
    "            state = next_state\n",
    "            raw_state = next_raw_state\n",
    "            \n",
    "            # Calculate and store current portfolio value\n",
    "            current_value = get_portfolio_value(raw_state, num_assets)\n",
    "            all_wealths[i, t + 1] = current_value\n",
    "            \n",
    "            if done:\n",
    "                # If done early, fill remaining steps with last value\n",
    "                all_wealths[i, t+2:] = current_value\n",
    "                break      \n",
    "    return all_wealths\n",
    "\n",
    "evaluation_wealths = evaluate_model(agent, env, manager)\n",
    "    \n",
    "    # Plot evaluation\n",
    "plot_evaluation(evaluation_wealths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
